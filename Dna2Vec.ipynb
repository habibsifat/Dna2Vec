{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dna2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QrqH2P22tGWL",
        "9Y-1UrMqtJ0p",
        "mgydp5IDtSiD",
        "8LuY8yLadUvm",
        "TZ4QHLsYfth2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/habibsifat/Dna2Vec/blob/master/Dna2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmJ-BBjzQmem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4QKDxa_ODV4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ed77a618-e02f-4599-997a-2481e2b6c490"
      },
      "source": [
        "from __future__ import print_function\n",
        "__author__ = 'maxim'\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "import string\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.utils.data_utils import get_file\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5IOYY2dHCMm",
        "colab_type": "code",
        "outputId": "9c7b27c1-d2b6-4843-bf7f-2629fc194503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "corpus_file = '/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70ds44.txt'\n",
        "print('\\nPreparing the  sentences...')\n",
        "sentences = []\n",
        "with open(corpus_file, 'r', encoding=\"utf-8-sig\") as f:\n",
        "    for line in f:\n",
        "        sentences.append(line.split())\n",
        "        \n",
        "print('Num sentences:', len(sentences))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Preparing the  sentences...\n",
            "Num sentences: 3023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ95trPAtAk2",
        "colab_type": "text"
      },
      "source": [
        "# **w2v**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2Q3vhIVNb8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QC1fd3PNX40",
        "colab_type": "code",
        "outputId": "6f70ffa0-0a58-46e6-8c78-0673d3d4471c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "print('\\nTraining word2vec...')\n",
        "word_model = Word2Vec(sentences, size=100, min_count=1, window=5, iter=10)\n",
        "#pretrained_weights = word_model.wv.syn0 \n",
        "#vocab_size, emdedding_size = pretrained_weights.shape\n",
        "#print('Result embedding shape:', pretrained_weights.shape)\n",
        "len(word_model.wv.vocab)\n",
        "#Save partly trained model\n",
        "#word_model.save('w2v_model.h5')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training word2vec...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "261"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZil4xClLDLN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "0b95cb4c-6818-4840-c70d-75fd1b5c841f"
      },
      "source": [
        "word_model.wv.vocab"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A': <gensim.models.keyedvectors.Vocab at 0x7f8fbbe9e978>,\n",
              " 'C': <gensim.models.keyedvectors.Vocab at 0x7f8fbbe9e908>,\n",
              " 'G': <gensim.models.keyedvectors.Vocab at 0x7f8fa5c68e48>,\n",
              " 'T': <gensim.models.keyedvectors.Vocab at 0x7f8f5bd8d358>,\n",
              " '\\ufeff': <gensim.models.keyedvectors.Vocab at 0x7f8f5bd8d2b0>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NbNY9qiRSpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_model.wv['\\ufeff']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbEpgfnnmcBx",
        "colab_type": "text"
      },
      "source": [
        "# **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATjrZ_uMNsPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_weights = word_model.wv.syn0 \n",
        "vocab_size, emdedding_size = pretrained_weights.shape\n",
        "print('Result embedding shape:', pretrained_weights.shape)\n",
        "print(vocab_size)\n",
        "print(emdedding_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5ijHh5JNsvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2idx(word):\n",
        "  return word_model.wv.vocab[word].index\n",
        "def idx2word(idx):\n",
        "  return word_model.wv.index2word[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndbf70tHN7xK",
        "colab_type": "code",
        "outputId": "913bff40-7ddd-4995-f193-62ab1dbfffaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "max_sentence_len = 78\n",
        "print('\\nPreparing the data for LSTM...')\n",
        "x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
        "#y = np.zeros([len(sentences)], dtype=np.int32)\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, word in enumerate(sentence[:-1]):\n",
        "    x[i, t] = word2idx(word)\n",
        "    #y[i] = word2idx(sentence[-1])\n",
        "print('x shape:', x.shape)\n",
        "#print('y shape:', y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Preparing the data for LSTM...\n",
            "x shape: (5720, 78)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbqHaYalOAIw",
        "colab_type": "code",
        "outputId": "d8d8a17a-d9ba-46b5-9d2b-e8dbf9f9aceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y = np.array([1]*2860 + [0]*2860)\n",
        "print('y shape:', y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y shape: (5720,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK8Im-QQOKd5",
        "colab_type": "code",
        "outputId": "48d338b1-04a0-475c-fdb5-00650f49596f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "#from sklearn.cross_validation import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.1,random_state=10)\n",
        "#shape of train and test objects\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "# shape of new y objects\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5148, 78)\n",
            "(572, 78)\n",
            "(5148,)\n",
            "(572,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrqH2P22tGWL",
        "colab_type": "text"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMPF8GREOLUb",
        "colab_type": "code",
        "outputId": "71597781-6315-4f15-905c-3df3b6127dff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from keras import optimizers\n",
        "print('\\nTraining LSTM...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
        "model.add(LSTM(units=512,return_sequences=True))\n",
        "#model.add(Activation('tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(units=512, return_sequences=False))\n",
        "#model.add(Activation('tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=vocab_size))\n",
        "model.add(Activation('softmax'))\n",
        "optz = optimizers.Adam(lr=1, clipnorm=5,beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model.compile(optimizer=optz, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training LSTM...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBbBSIaoORyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, Y_train,batch_size=128,epochs=5,shuffle=True, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y-1UrMqtJ0p",
        "colab_type": "text"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8OIrBiyPxmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Convolution1D, Dense, Dropout, Flatten\n",
        "from keras.optimizers import Adam,RMSprop\n",
        "from keras.callbacks import EarlyStopping\n",
        "model = Sequential()\n",
        "#model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
        "#model.add(Embedding(4, 100, input_length=81))\n",
        "model.add(Convolution1D(200, kernel_size=3, activation='relu', input_shape=(81, 1), name='m1_conv1'))\n",
        "#model.add(Dropout(0.5, name='m1_drop1'))\n",
        "model.add(Convolution1D(128, kernel_size=3, activation='relu', name='m1_conv2'))\n",
        "#model.add(Dropout(0.5, name='m1_drop2'))\n",
        "model.add(Convolution1D(64, kernel_size=3, activation='relu', name='m1_conv3'))\n",
        "#model.add(Dropout(0.5, name='m1_drop3'))\n",
        "model.add(Flatten(name='m1_flatten1'))\n",
        "#model.add(Dropout(0.5, name='m1_drop4'))\n",
        "model.add(Dense(64, activation='relu', name='m1_dense1'))\n",
        "model.add(Dense(1, activation='softmax', name='m1_dense2'))\n",
        "\n",
        "#model.compile(RMSprop(lr=.003), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(Adam(lr=.003), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', patience=16)\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=5, batch_size=32, callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcsZEWNAtPSk",
        "colab_type": "text"
      },
      "source": [
        "# 1-Mer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAkPNcNut9WP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/promoter70.txt')\n",
        "seqs = df.iloc[:, 0].tolist()\n",
        "\n",
        "for i in range(len(seqs)):\n",
        "  seqs[i] = seqs[i].upper()\n",
        "  \n",
        "with open('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70ds11.txt', 'w', encoding=\"utf-8-sig\") as f:\n",
        "  for j in seqs:\n",
        "    for k in j:\n",
        "      f.write(k)\n",
        "      f.write(\" \")\n",
        "    f.write(\"\\n\")\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvLcDjD3sxbH",
        "colab_type": "code",
        "outputId": "b2bd6599-0ab3-4130-d8a1-8a1a27851e19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNnaRy4xv-sS",
        "colab_type": "text"
      },
      "source": [
        "**this cell**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nWoq8YqTtCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A4 = np.zeros([2860,81,100],dtype=np.int32)\n",
        "\n",
        "for i in range(0,len(sentences)): \n",
        "  p = 0\n",
        "  for j in sentences[i]:\n",
        "    q = 0\n",
        "    for k in word_model.wv[j]:\n",
        "      r = 0\n",
        "      A4[p][q][r] = k\n",
        "      r=r+1\n",
        "    q=q+1\n",
        "  p=p+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL21eaWl4K7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#second dimesion of the array 81 because there are total 81 1-mers\n",
        "A2 = np.zeros([3023,79,100])\n",
        "\n",
        "for current_sequence_number in range(0,len(sentences)):\n",
        "\n",
        "  kmer_counter = 0\n",
        "  current_sequence = sentences[current_sequence_number] \n",
        "  \n",
        "  #here j contains a k-mer from current_sequence\n",
        "\n",
        "  for kmer in current_sequence:\n",
        "    \n",
        "    A2[current_sequence_number,kmer_counter,:] = word_model.wv[kmer] \n",
        "\n",
        "    kmer_counter = kmer_counter + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9dSLTQ5ckst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70A44.npy',A2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GawrGwYub0Ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A2[1][1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b6mzbMP4rPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_model.wv['CGCG']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uga9madSb3Qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences[1][1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgydp5IDtSiD",
        "colab_type": "text"
      },
      "source": [
        "# 2-Mer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiqRFi_AcAgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/promoter70.txt')\n",
        "seqs = df.iloc[:, 0].tolist()\n",
        "\n",
        "for i in range(len(seqs)):\n",
        "  seqs[i] = seqs[i].upper()\n",
        "  \n",
        "with open('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70ds22.txt', 'w', encoding=\"utf-8-sig\") as f:#\n",
        "  for j in seqs:\n",
        "    for k in range(0,len(j)-1):\n",
        "      #print(j[k],end='')\n",
        "      #print(j[k+1],end=' ')\n",
        "      f.write(j[k])\n",
        "      f.write(j[k+1])\n",
        "      f.write(\" \")\n",
        "    f.write(\"\\n\")\n",
        "    #print()\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBIwrHZH409o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LuY8yLadUvm",
        "colab_type": "text"
      },
      "source": [
        "# **3-Mer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q9TAJvsdYkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/promoter70.txt')\n",
        "seqs = df.iloc[:, 0].tolist()\n",
        "\n",
        "for i in range(len(seqs)):\n",
        "  seqs[i] = seqs[i].upper()\n",
        "  \n",
        "with open('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70ds33.txt', 'w', encoding=\"utf-8-sig\") as f:#\n",
        "  for j in seqs:\n",
        "    for k in range(0,len(j)-2):\n",
        "      #print(j[k],end='')\n",
        "      #print(j[k+1],end=' ')\n",
        "      f.write(j[k])\n",
        "      f.write(j[k+1])\n",
        "      f.write(j[k+2])\n",
        "      f.write(\" \")\n",
        "    f.write(\"\\n\")\n",
        "    #print()\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ4QHLsYfth2",
        "colab_type": "text"
      },
      "source": [
        "# **4-Mer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY8LsQikfxOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/promoter70.txt')\n",
        "seqs = df.iloc[:, 0].tolist()\n",
        "\n",
        "for i in range(len(seqs)):\n",
        "  seqs[i] = seqs[i].upper()\n",
        "  \n",
        "with open('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70ds44.txt', 'w', encoding=\"utf-8-sig\") as f:#\n",
        "  for j in seqs:\n",
        "    for k in range(0,len(j)-3):\n",
        "      #print(j[k],end='')\n",
        "      #print(j[k+1],end=' ')\n",
        "      f.write(j[k])\n",
        "      f.write(j[k+1])\n",
        "      f.write(j[k+2])\n",
        "      f.write(j[k+3])\n",
        "      f.write(\" \")\n",
        "    f.write(\"\\n\")\n",
        "    #print()\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE4VYcEKhYhg",
        "colab_type": "text"
      },
      "source": [
        "# **MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4vLEkfJ8g64",
        "colab_type": "code",
        "outputId": "a132d34a-ec58-403e-cbab-84bd030d98e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "X1 = np.load('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70A11.npy')\n",
        "X2 = np.load('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70A22.npy')\n",
        "X3 = np.load('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70A33.npy')\n",
        "X4 = np.load('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70A44.npy')\n",
        "print('X shape:', X1.shape)\n",
        "y = np.array([1]*1694 + [0]*1329)\n",
        "print('y shape:', y.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape: (3023, 82, 100)\n",
            "y shape: (3023,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dPAOYHw9QLA",
        "colab_type": "code",
        "outputId": "4383be3d-aaa9-4bb9-be65-9e93dfb28651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train1,X_test1,Y_train,Y_test = train_test_split(X1,y,test_size=0.1,random_state=10)\n",
        "X_train2,X_test2,Y_train,Y_test = train_test_split(X2,y,test_size=0.1,random_state=10)\n",
        "X_train3,X_test3,Y_train,Y_test = train_test_split(X3,y,test_size=0.1,random_state=10)\n",
        "X_train4,X_test4,Y_train,Y_test = train_test_split(X4,y,test_size=0.1,random_state=10)\n",
        "#shape of train and test objects\n",
        "print(X_train1.shape)\n",
        "print(X_test1.shape)\n",
        "# shape of new y objects\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2720, 82, 100)\n",
            "(303, 82, 100)\n",
            "(2720,)\n",
            "(303,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeDf_EpHhcNc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95e81ee8-6d11-42f8-94ab-4640f3e536ea"
      },
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Convolution1D, Dense, Dropout, Flatten, Conv1D, concatenate\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "inputA = Input(shape=(82,100)) # One\n",
        "inputB = Input(shape=(81,100)) # two\n",
        "inputC = Input(shape=(80,100)) # Three\n",
        "inputD = Input(shape=(79,100)) # Four\n",
        "\n",
        "a = Convolution1D(128, kernel_size=3, activation='relu', padding='same') (inputA)\n",
        "a = Dropout(0.5) (a)\n",
        "a = Convolution1D(64, kernel_size=3, activation='relu', padding='same') (a)\n",
        "a = Dropout(0.5) (a)\n",
        "a = Convolution1D(32, kernel_size=3, activation='relu', padding='same') (a)\n",
        "a = Dropout(0.5) (a)\n",
        "a = Flatten() (a)\n",
        "\n",
        "b = Convolution1D(filters = 128, kernel_size= 3, activation='relu', padding='same') (inputB)\n",
        "b = Dropout(0.50) (b)\n",
        "b = Convolution1D(filters = 64, kernel_size = 3, activation='relu', padding='same') (b)\n",
        "b = Dropout(0.50) (b)\n",
        "b = Convolution1D(filters = 32, kernel_size = 3, activation='relu', padding='same') (b)\n",
        "b = Dropout(0.50) (b)\n",
        "b = Flatten() (b)\n",
        "\n",
        "c = Convolution1D(256, kernel_size=3, activation='relu', padding='same') (inputC)\n",
        "c = Dropout(0.5) (c)\n",
        "c = Conv1D(128, kernel_size=3, activation='relu', padding='same') (c)\n",
        "c = Dropout(0.5) (c)\n",
        "c = Conv1D(64, kernel_size=3, activation='relu', padding='same') (c)\n",
        "c = Dropout(0.5) (c)\n",
        "c = Conv1D(32, kernel_size=3, activation='relu', padding='same') (c)\n",
        "c = Dropout(0.5) (c)\n",
        "c = Flatten() (c)\n",
        "\n",
        "d = Convolution1D(128, kernel_size=3, activation='relu', padding='same') (inputD)\n",
        "d = Dropout(0.5) (d)\n",
        "d = Conv1D(64, kernel_size=3, activation='relu', padding='same') (d)\n",
        "d = Dropout(0.5) (d)\n",
        "d = Conv1D(32, kernel_size=3, activation='relu', padding='same') (d)\n",
        "d = Dropout(0.5) (d)\n",
        "d = Flatten() (d)\n",
        "\n",
        "combined = concatenate([a, b, c, d])\n",
        "\n",
        "h = Dropout(0.5) (combined)\n",
        "h = Dense(128, activation='relu') (h)\n",
        "h = Dropout(0.5) (h)\n",
        "h = Dense(64, activation='relu') (h)\n",
        "output = Dense(1, activation='sigmoid') (h)\n",
        "\n",
        "model = Model(inputs=[inputA, inputB, inputC, inputD], outputs=output)\n",
        "model.compile(Adam(lr=0.0003), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit([X_train1, X_train2, X_train3, X_train4], Y_train, validation_data=([X_test1, X_test2, X_test3, X_test4], Y_test),\n",
        "          epochs=200, batch_size=32, verbose=1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2720 samples, validate on 303 samples\n",
            "Epoch 1/200\n",
            "2720/2720 [==============================] - 3s 992us/step - loss: 0.7738 - accuracy: 0.5279 - val_loss: 0.6887 - val_accuracy: 0.5479\n",
            "Epoch 2/200\n",
            "2720/2720 [==============================] - 2s 594us/step - loss: 0.7130 - accuracy: 0.5243 - val_loss: 0.6887 - val_accuracy: 0.5644\n",
            "Epoch 3/200\n",
            "2720/2720 [==============================] - 2s 596us/step - loss: 0.7124 - accuracy: 0.5272 - val_loss: 0.6892 - val_accuracy: 0.5578\n",
            "Epoch 4/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.6976 - accuracy: 0.5449 - val_loss: 0.6836 - val_accuracy: 0.5710\n",
            "Epoch 5/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.6931 - accuracy: 0.5599 - val_loss: 0.6731 - val_accuracy: 0.6139\n",
            "Epoch 6/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.6790 - accuracy: 0.5710 - val_loss: 0.6344 - val_accuracy: 0.6205\n",
            "Epoch 7/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.6510 - accuracy: 0.6235 - val_loss: 0.6035 - val_accuracy: 0.6634\n",
            "Epoch 8/200\n",
            "2720/2720 [==============================] - 2s 584us/step - loss: 0.6253 - accuracy: 0.6544 - val_loss: 0.5743 - val_accuracy: 0.6898\n",
            "Epoch 9/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.5944 - accuracy: 0.6893 - val_loss: 0.5549 - val_accuracy: 0.6997\n",
            "Epoch 10/200\n",
            "2720/2720 [==============================] - 2s 584us/step - loss: 0.5737 - accuracy: 0.7132 - val_loss: 0.5275 - val_accuracy: 0.7426\n",
            "Epoch 11/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.5593 - accuracy: 0.7180 - val_loss: 0.5070 - val_accuracy: 0.7657\n",
            "Epoch 12/200\n",
            "2720/2720 [==============================] - 2s 573us/step - loss: 0.5378 - accuracy: 0.7390 - val_loss: 0.4935 - val_accuracy: 0.7591\n",
            "Epoch 13/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.5235 - accuracy: 0.7415 - val_loss: 0.4747 - val_accuracy: 0.7822\n",
            "Epoch 14/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.5252 - accuracy: 0.7430 - val_loss: 0.4693 - val_accuracy: 0.8086\n",
            "Epoch 15/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.4958 - accuracy: 0.7665 - val_loss: 0.4673 - val_accuracy: 0.7855\n",
            "Epoch 16/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.5112 - accuracy: 0.7588 - val_loss: 0.4572 - val_accuracy: 0.8251\n",
            "Epoch 17/200\n",
            "2720/2720 [==============================] - 2s 575us/step - loss: 0.4913 - accuracy: 0.7621 - val_loss: 0.4460 - val_accuracy: 0.8119\n",
            "Epoch 18/200\n",
            "2720/2720 [==============================] - 2s 588us/step - loss: 0.4829 - accuracy: 0.7735 - val_loss: 0.4546 - val_accuracy: 0.8020\n",
            "Epoch 19/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.4787 - accuracy: 0.7743 - val_loss: 0.4311 - val_accuracy: 0.8086\n",
            "Epoch 20/200\n",
            "2720/2720 [==============================] - 2s 589us/step - loss: 0.4842 - accuracy: 0.7732 - val_loss: 0.4363 - val_accuracy: 0.7954\n",
            "Epoch 21/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.4599 - accuracy: 0.7879 - val_loss: 0.4460 - val_accuracy: 0.7756\n",
            "Epoch 22/200\n",
            "2720/2720 [==============================] - 2s 570us/step - loss: 0.4589 - accuracy: 0.7926 - val_loss: 0.4357 - val_accuracy: 0.7987\n",
            "Epoch 23/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.4507 - accuracy: 0.7945 - val_loss: 0.4285 - val_accuracy: 0.7888\n",
            "Epoch 24/200\n",
            "2720/2720 [==============================] - 2s 568us/step - loss: 0.4470 - accuracy: 0.7923 - val_loss: 0.4098 - val_accuracy: 0.8119\n",
            "Epoch 25/200\n",
            "2720/2720 [==============================] - 2s 589us/step - loss: 0.4434 - accuracy: 0.7912 - val_loss: 0.4018 - val_accuracy: 0.8185\n",
            "Epoch 26/200\n",
            "2720/2720 [==============================] - 2s 576us/step - loss: 0.4347 - accuracy: 0.7971 - val_loss: 0.4085 - val_accuracy: 0.8218\n",
            "Epoch 27/200\n",
            "2720/2720 [==============================] - 2s 576us/step - loss: 0.4270 - accuracy: 0.8026 - val_loss: 0.4020 - val_accuracy: 0.8251\n",
            "Epoch 28/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.4138 - accuracy: 0.8132 - val_loss: 0.4002 - val_accuracy: 0.8152\n",
            "Epoch 29/200\n",
            "2720/2720 [==============================] - 2s 584us/step - loss: 0.4144 - accuracy: 0.8081 - val_loss: 0.3911 - val_accuracy: 0.8317\n",
            "Epoch 30/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.4143 - accuracy: 0.8143 - val_loss: 0.3991 - val_accuracy: 0.8251\n",
            "Epoch 31/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.4090 - accuracy: 0.8191 - val_loss: 0.3914 - val_accuracy: 0.8284\n",
            "Epoch 32/200\n",
            "2720/2720 [==============================] - 2s 586us/step - loss: 0.3933 - accuracy: 0.8250 - val_loss: 0.3882 - val_accuracy: 0.8284\n",
            "Epoch 33/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.3931 - accuracy: 0.8246 - val_loss: 0.3948 - val_accuracy: 0.8515\n",
            "Epoch 34/200\n",
            "2720/2720 [==============================] - 2s 586us/step - loss: 0.3904 - accuracy: 0.8261 - val_loss: 0.3989 - val_accuracy: 0.8317\n",
            "Epoch 35/200\n",
            "2720/2720 [==============================] - 2s 571us/step - loss: 0.3872 - accuracy: 0.8261 - val_loss: 0.3965 - val_accuracy: 0.8416\n",
            "Epoch 36/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.3888 - accuracy: 0.8199 - val_loss: 0.3785 - val_accuracy: 0.8383\n",
            "Epoch 37/200\n",
            "2720/2720 [==============================] - 2s 587us/step - loss: 0.3818 - accuracy: 0.8257 - val_loss: 0.3751 - val_accuracy: 0.8350\n",
            "Epoch 38/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.3817 - accuracy: 0.8254 - val_loss: 0.3764 - val_accuracy: 0.8449\n",
            "Epoch 39/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.3688 - accuracy: 0.8360 - val_loss: 0.3747 - val_accuracy: 0.8383\n",
            "Epoch 40/200\n",
            "2720/2720 [==============================] - 2s 588us/step - loss: 0.3639 - accuracy: 0.8342 - val_loss: 0.3762 - val_accuracy: 0.8251\n",
            "Epoch 41/200\n",
            "2720/2720 [==============================] - 2s 595us/step - loss: 0.3644 - accuracy: 0.8346 - val_loss: 0.3678 - val_accuracy: 0.8251\n",
            "Epoch 42/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.3657 - accuracy: 0.8419 - val_loss: 0.3633 - val_accuracy: 0.8449\n",
            "Epoch 43/200\n",
            "2720/2720 [==============================] - 2s 588us/step - loss: 0.3578 - accuracy: 0.8390 - val_loss: 0.3660 - val_accuracy: 0.8548\n",
            "Epoch 44/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.3580 - accuracy: 0.8489 - val_loss: 0.3660 - val_accuracy: 0.8515\n",
            "Epoch 45/200\n",
            "2720/2720 [==============================] - 2s 592us/step - loss: 0.3509 - accuracy: 0.8441 - val_loss: 0.3739 - val_accuracy: 0.8548\n",
            "Epoch 46/200\n",
            "2720/2720 [==============================] - 2s 589us/step - loss: 0.3379 - accuracy: 0.8474 - val_loss: 0.3692 - val_accuracy: 0.8251\n",
            "Epoch 47/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.3543 - accuracy: 0.8456 - val_loss: 0.3518 - val_accuracy: 0.8515\n",
            "Epoch 48/200\n",
            "2720/2720 [==============================] - 2s 592us/step - loss: 0.3477 - accuracy: 0.8518 - val_loss: 0.3604 - val_accuracy: 0.8581\n",
            "Epoch 49/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.3302 - accuracy: 0.8555 - val_loss: 0.3562 - val_accuracy: 0.8482\n",
            "Epoch 50/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.3356 - accuracy: 0.8529 - val_loss: 0.3626 - val_accuracy: 0.8680\n",
            "Epoch 51/200\n",
            "2720/2720 [==============================] - 2s 592us/step - loss: 0.3294 - accuracy: 0.8526 - val_loss: 0.3515 - val_accuracy: 0.8647\n",
            "Epoch 52/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.3199 - accuracy: 0.8607 - val_loss: 0.3484 - val_accuracy: 0.8680\n",
            "Epoch 53/200\n",
            "2720/2720 [==============================] - 2s 589us/step - loss: 0.3279 - accuracy: 0.8570 - val_loss: 0.3570 - val_accuracy: 0.8548\n",
            "Epoch 54/200\n",
            "2720/2720 [==============================] - 2s 584us/step - loss: 0.3181 - accuracy: 0.8596 - val_loss: 0.3518 - val_accuracy: 0.8515\n",
            "Epoch 55/200\n",
            "2720/2720 [==============================] - 2s 568us/step - loss: 0.3072 - accuracy: 0.8687 - val_loss: 0.3445 - val_accuracy: 0.8713\n",
            "Epoch 56/200\n",
            "2720/2720 [==============================] - 2s 569us/step - loss: 0.3063 - accuracy: 0.8629 - val_loss: 0.3487 - val_accuracy: 0.8647\n",
            "Epoch 57/200\n",
            "2720/2720 [==============================] - 2s 574us/step - loss: 0.2938 - accuracy: 0.8768 - val_loss: 0.3418 - val_accuracy: 0.8383\n",
            "Epoch 58/200\n",
            "2720/2720 [==============================] - 2s 573us/step - loss: 0.3113 - accuracy: 0.8632 - val_loss: 0.3432 - val_accuracy: 0.8515\n",
            "Epoch 59/200\n",
            "2720/2720 [==============================] - 2s 592us/step - loss: 0.2974 - accuracy: 0.8691 - val_loss: 0.3350 - val_accuracy: 0.8614\n",
            "Epoch 60/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.2927 - accuracy: 0.8754 - val_loss: 0.3337 - val_accuracy: 0.8713\n",
            "Epoch 61/200\n",
            "2720/2720 [==============================] - 2s 586us/step - loss: 0.2991 - accuracy: 0.8640 - val_loss: 0.3274 - val_accuracy: 0.8779\n",
            "Epoch 62/200\n",
            "2720/2720 [==============================] - 2s 569us/step - loss: 0.2984 - accuracy: 0.8754 - val_loss: 0.3290 - val_accuracy: 0.8779\n",
            "Epoch 63/200\n",
            "2720/2720 [==============================] - 2s 589us/step - loss: 0.2714 - accuracy: 0.8827 - val_loss: 0.3233 - val_accuracy: 0.8779\n",
            "Epoch 64/200\n",
            "2720/2720 [==============================] - 2s 568us/step - loss: 0.2797 - accuracy: 0.8794 - val_loss: 0.3232 - val_accuracy: 0.8713\n",
            "Epoch 65/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.2773 - accuracy: 0.8835 - val_loss: 0.3289 - val_accuracy: 0.8746\n",
            "Epoch 66/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.2860 - accuracy: 0.8765 - val_loss: 0.3286 - val_accuracy: 0.8713\n",
            "Epoch 67/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.2764 - accuracy: 0.8871 - val_loss: 0.3139 - val_accuracy: 0.8713\n",
            "Epoch 68/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.2732 - accuracy: 0.8849 - val_loss: 0.3181 - val_accuracy: 0.8713\n",
            "Epoch 69/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.2646 - accuracy: 0.8886 - val_loss: 0.3209 - val_accuracy: 0.8779\n",
            "Epoch 70/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.2578 - accuracy: 0.8904 - val_loss: 0.3159 - val_accuracy: 0.8713\n",
            "Epoch 71/200\n",
            "2720/2720 [==============================] - 2s 576us/step - loss: 0.2554 - accuracy: 0.8904 - val_loss: 0.3081 - val_accuracy: 0.8812\n",
            "Epoch 72/200\n",
            "2720/2720 [==============================] - 2s 567us/step - loss: 0.2374 - accuracy: 0.8989 - val_loss: 0.3033 - val_accuracy: 0.8878\n",
            "Epoch 73/200\n",
            "2720/2720 [==============================] - 2s 575us/step - loss: 0.2358 - accuracy: 0.8996 - val_loss: 0.3012 - val_accuracy: 0.8812\n",
            "Epoch 74/200\n",
            "2720/2720 [==============================] - 2s 570us/step - loss: 0.2376 - accuracy: 0.8974 - val_loss: 0.3071 - val_accuracy: 0.8746\n",
            "Epoch 75/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.2373 - accuracy: 0.8985 - val_loss: 0.3124 - val_accuracy: 0.8878\n",
            "Epoch 76/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.2284 - accuracy: 0.9070 - val_loss: 0.2907 - val_accuracy: 0.8878\n",
            "Epoch 77/200\n",
            "2720/2720 [==============================] - 2s 590us/step - loss: 0.2258 - accuracy: 0.9081 - val_loss: 0.3010 - val_accuracy: 0.8845\n",
            "Epoch 78/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.2285 - accuracy: 0.9026 - val_loss: 0.2843 - val_accuracy: 0.8779\n",
            "Epoch 79/200\n",
            "2720/2720 [==============================] - 2s 587us/step - loss: 0.2290 - accuracy: 0.9007 - val_loss: 0.2960 - val_accuracy: 0.8911\n",
            "Epoch 80/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.2266 - accuracy: 0.9070 - val_loss: 0.2982 - val_accuracy: 0.8911\n",
            "Epoch 81/200\n",
            "2720/2720 [==============================] - 2s 587us/step - loss: 0.2143 - accuracy: 0.9121 - val_loss: 0.2917 - val_accuracy: 0.8911\n",
            "Epoch 82/200\n",
            "2720/2720 [==============================] - 2s 574us/step - loss: 0.2075 - accuracy: 0.9202 - val_loss: 0.2988 - val_accuracy: 0.8845\n",
            "Epoch 83/200\n",
            "2720/2720 [==============================] - 2s 573us/step - loss: 0.2233 - accuracy: 0.9114 - val_loss: 0.3031 - val_accuracy: 0.8779\n",
            "Epoch 84/200\n",
            "2720/2720 [==============================] - 2s 576us/step - loss: 0.2177 - accuracy: 0.9103 - val_loss: 0.2843 - val_accuracy: 0.8878\n",
            "Epoch 85/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.2085 - accuracy: 0.9118 - val_loss: 0.2853 - val_accuracy: 0.8944\n",
            "Epoch 86/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.1999 - accuracy: 0.9202 - val_loss: 0.2865 - val_accuracy: 0.8911\n",
            "Epoch 87/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.2158 - accuracy: 0.9147 - val_loss: 0.2890 - val_accuracy: 0.8878\n",
            "Epoch 88/200\n",
            "2720/2720 [==============================] - 2s 572us/step - loss: 0.2049 - accuracy: 0.9162 - val_loss: 0.2903 - val_accuracy: 0.8977\n",
            "Epoch 89/200\n",
            "2720/2720 [==============================] - 2s 571us/step - loss: 0.2014 - accuracy: 0.9162 - val_loss: 0.2807 - val_accuracy: 0.8911\n",
            "Epoch 90/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.1933 - accuracy: 0.9239 - val_loss: 0.2947 - val_accuracy: 0.8812\n",
            "Epoch 91/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.1915 - accuracy: 0.9180 - val_loss: 0.2988 - val_accuracy: 0.8911\n",
            "Epoch 92/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.1826 - accuracy: 0.9261 - val_loss: 0.2937 - val_accuracy: 0.8878\n",
            "Epoch 93/200\n",
            "2720/2720 [==============================] - 2s 573us/step - loss: 0.1986 - accuracy: 0.9184 - val_loss: 0.2952 - val_accuracy: 0.8779\n",
            "Epoch 94/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.1942 - accuracy: 0.9235 - val_loss: 0.2870 - val_accuracy: 0.8944\n",
            "Epoch 95/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.1719 - accuracy: 0.9283 - val_loss: 0.2812 - val_accuracy: 0.8944\n",
            "Epoch 96/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.1824 - accuracy: 0.9290 - val_loss: 0.2942 - val_accuracy: 0.8812\n",
            "Epoch 97/200\n",
            "2720/2720 [==============================] - 2s 571us/step - loss: 0.1957 - accuracy: 0.9224 - val_loss: 0.2776 - val_accuracy: 0.9043\n",
            "Epoch 98/200\n",
            "2720/2720 [==============================] - 2s 591us/step - loss: 0.1918 - accuracy: 0.9246 - val_loss: 0.3039 - val_accuracy: 0.8911\n",
            "Epoch 99/200\n",
            "2720/2720 [==============================] - 2s 584us/step - loss: 0.1787 - accuracy: 0.9254 - val_loss: 0.2774 - val_accuracy: 0.8878\n",
            "Epoch 100/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.1667 - accuracy: 0.9294 - val_loss: 0.2812 - val_accuracy: 0.8911\n",
            "Epoch 101/200\n",
            "2720/2720 [==============================] - 2s 576us/step - loss: 0.1708 - accuracy: 0.9294 - val_loss: 0.2897 - val_accuracy: 0.8944\n",
            "Epoch 102/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.1584 - accuracy: 0.9338 - val_loss: 0.2693 - val_accuracy: 0.9010\n",
            "Epoch 103/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.1770 - accuracy: 0.9309 - val_loss: 0.2850 - val_accuracy: 0.8977\n",
            "Epoch 104/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.1666 - accuracy: 0.9353 - val_loss: 0.2832 - val_accuracy: 0.8944\n",
            "Epoch 105/200\n",
            "2720/2720 [==============================] - 2s 576us/step - loss: 0.1567 - accuracy: 0.9423 - val_loss: 0.2787 - val_accuracy: 0.9010\n",
            "Epoch 106/200\n",
            "2720/2720 [==============================] - 2s 574us/step - loss: 0.1715 - accuracy: 0.9320 - val_loss: 0.2972 - val_accuracy: 0.8944\n",
            "Epoch 107/200\n",
            "2720/2720 [==============================] - 2s 561us/step - loss: 0.1621 - accuracy: 0.9353 - val_loss: 0.2720 - val_accuracy: 0.9010\n",
            "Epoch 108/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.1549 - accuracy: 0.9364 - val_loss: 0.3134 - val_accuracy: 0.8878\n",
            "Epoch 109/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.1542 - accuracy: 0.9375 - val_loss: 0.2784 - val_accuracy: 0.8845\n",
            "Epoch 110/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.1504 - accuracy: 0.9408 - val_loss: 0.2737 - val_accuracy: 0.9010\n",
            "Epoch 111/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.1516 - accuracy: 0.9452 - val_loss: 0.2710 - val_accuracy: 0.9076\n",
            "Epoch 112/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.1604 - accuracy: 0.9357 - val_loss: 0.2734 - val_accuracy: 0.8911\n",
            "Epoch 113/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.1447 - accuracy: 0.9401 - val_loss: 0.2751 - val_accuracy: 0.9010\n",
            "Epoch 114/200\n",
            "2720/2720 [==============================] - 2s 576us/step - loss: 0.1510 - accuracy: 0.9430 - val_loss: 0.2670 - val_accuracy: 0.8977\n",
            "Epoch 115/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.1464 - accuracy: 0.9415 - val_loss: 0.2750 - val_accuracy: 0.9142\n",
            "Epoch 116/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.1340 - accuracy: 0.9500 - val_loss: 0.2958 - val_accuracy: 0.8911\n",
            "Epoch 117/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.1421 - accuracy: 0.9474 - val_loss: 0.2782 - val_accuracy: 0.8944\n",
            "Epoch 118/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.1516 - accuracy: 0.9419 - val_loss: 0.2841 - val_accuracy: 0.9043\n",
            "Epoch 119/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.1433 - accuracy: 0.9452 - val_loss: 0.2990 - val_accuracy: 0.8911\n",
            "Epoch 120/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.1319 - accuracy: 0.9493 - val_loss: 0.2868 - val_accuracy: 0.9076\n",
            "Epoch 121/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.1407 - accuracy: 0.9412 - val_loss: 0.2847 - val_accuracy: 0.9109\n",
            "Epoch 122/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.1381 - accuracy: 0.9456 - val_loss: 0.2816 - val_accuracy: 0.9175\n",
            "Epoch 123/200\n",
            "2720/2720 [==============================] - 2s 572us/step - loss: 0.1324 - accuracy: 0.9511 - val_loss: 0.2729 - val_accuracy: 0.9175\n",
            "Epoch 124/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.1230 - accuracy: 0.9518 - val_loss: 0.2748 - val_accuracy: 0.9010\n",
            "Epoch 125/200\n",
            "2720/2720 [==============================] - 2s 561us/step - loss: 0.1347 - accuracy: 0.9456 - val_loss: 0.2991 - val_accuracy: 0.8977\n",
            "Epoch 126/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.1328 - accuracy: 0.9489 - val_loss: 0.2737 - val_accuracy: 0.9109\n",
            "Epoch 127/200\n",
            "2720/2720 [==============================] - 2s 573us/step - loss: 0.1167 - accuracy: 0.9537 - val_loss: 0.2929 - val_accuracy: 0.9109\n",
            "Epoch 128/200\n",
            "2720/2720 [==============================] - 2s 588us/step - loss: 0.1205 - accuracy: 0.9507 - val_loss: 0.2891 - val_accuracy: 0.9043\n",
            "Epoch 129/200\n",
            "2720/2720 [==============================] - 2s 565us/step - loss: 0.1482 - accuracy: 0.9382 - val_loss: 0.2663 - val_accuracy: 0.9109\n",
            "Epoch 130/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.1399 - accuracy: 0.9456 - val_loss: 0.2798 - val_accuracy: 0.9076\n",
            "Epoch 131/200\n",
            "2720/2720 [==============================] - 2s 574us/step - loss: 0.1224 - accuracy: 0.9482 - val_loss: 0.2774 - val_accuracy: 0.8944\n",
            "Epoch 132/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.1174 - accuracy: 0.9559 - val_loss: 0.2744 - val_accuracy: 0.9043\n",
            "Epoch 133/200\n",
            "2720/2720 [==============================] - 2s 575us/step - loss: 0.1239 - accuracy: 0.9522 - val_loss: 0.2815 - val_accuracy: 0.9076\n",
            "Epoch 134/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.1298 - accuracy: 0.9511 - val_loss: 0.2714 - val_accuracy: 0.9175\n",
            "Epoch 135/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.1165 - accuracy: 0.9533 - val_loss: 0.2802 - val_accuracy: 0.9142\n",
            "Epoch 136/200\n",
            "2720/2720 [==============================] - 2s 573us/step - loss: 0.1125 - accuracy: 0.9566 - val_loss: 0.2934 - val_accuracy: 0.9043\n",
            "Epoch 137/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.1052 - accuracy: 0.9585 - val_loss: 0.3212 - val_accuracy: 0.8977\n",
            "Epoch 138/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.1126 - accuracy: 0.9566 - val_loss: 0.2837 - val_accuracy: 0.9076\n",
            "Epoch 139/200\n",
            "2720/2720 [==============================] - 2s 584us/step - loss: 0.1200 - accuracy: 0.9544 - val_loss: 0.2892 - val_accuracy: 0.9043\n",
            "Epoch 140/200\n",
            "2720/2720 [==============================] - 2s 595us/step - loss: 0.0874 - accuracy: 0.9669 - val_loss: 0.2937 - val_accuracy: 0.9109\n",
            "Epoch 141/200\n",
            "2720/2720 [==============================] - 2s 595us/step - loss: 0.1081 - accuracy: 0.9555 - val_loss: 0.3035 - val_accuracy: 0.8977\n",
            "Epoch 142/200\n",
            "2720/2720 [==============================] - 2s 599us/step - loss: 0.0885 - accuracy: 0.9662 - val_loss: 0.2767 - val_accuracy: 0.9142\n",
            "Epoch 143/200\n",
            "2720/2720 [==============================] - 2s 568us/step - loss: 0.1090 - accuracy: 0.9533 - val_loss: 0.3187 - val_accuracy: 0.8911\n",
            "Epoch 144/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.1150 - accuracy: 0.9529 - val_loss: 0.2626 - val_accuracy: 0.9142\n",
            "Epoch 145/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.0900 - accuracy: 0.9636 - val_loss: 0.2938 - val_accuracy: 0.8977\n",
            "Epoch 146/200\n",
            "2720/2720 [==============================] - 2s 594us/step - loss: 0.1039 - accuracy: 0.9588 - val_loss: 0.2902 - val_accuracy: 0.9010\n",
            "Epoch 147/200\n",
            "2720/2720 [==============================] - 2s 575us/step - loss: 0.0963 - accuracy: 0.9621 - val_loss: 0.2899 - val_accuracy: 0.9043\n",
            "Epoch 148/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.1006 - accuracy: 0.9669 - val_loss: 0.2956 - val_accuracy: 0.9076\n",
            "Epoch 149/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.0887 - accuracy: 0.9654 - val_loss: 0.2818 - val_accuracy: 0.9142\n",
            "Epoch 150/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.0918 - accuracy: 0.9665 - val_loss: 0.2954 - val_accuracy: 0.9109\n",
            "Epoch 151/200\n",
            "2720/2720 [==============================] - 2s 587us/step - loss: 0.1072 - accuracy: 0.9599 - val_loss: 0.3026 - val_accuracy: 0.9010\n",
            "Epoch 152/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.1053 - accuracy: 0.9603 - val_loss: 0.2942 - val_accuracy: 0.8977\n",
            "Epoch 153/200\n",
            "2720/2720 [==============================] - 2s 570us/step - loss: 0.1031 - accuracy: 0.9596 - val_loss: 0.2912 - val_accuracy: 0.8944\n",
            "Epoch 154/200\n",
            "2720/2720 [==============================] - 2s 573us/step - loss: 0.0949 - accuracy: 0.9665 - val_loss: 0.2876 - val_accuracy: 0.9043\n",
            "Epoch 155/200\n",
            "2720/2720 [==============================] - 2s 584us/step - loss: 0.1110 - accuracy: 0.9559 - val_loss: 0.2625 - val_accuracy: 0.9043\n",
            "Epoch 156/200\n",
            "2720/2720 [==============================] - 2s 584us/step - loss: 0.0955 - accuracy: 0.9625 - val_loss: 0.2627 - val_accuracy: 0.9175\n",
            "Epoch 157/200\n",
            "2720/2720 [==============================] - 2s 572us/step - loss: 0.1030 - accuracy: 0.9625 - val_loss: 0.2588 - val_accuracy: 0.9076\n",
            "Epoch 158/200\n",
            "2720/2720 [==============================] - 2s 587us/step - loss: 0.1005 - accuracy: 0.9603 - val_loss: 0.2983 - val_accuracy: 0.9010\n",
            "Epoch 159/200\n",
            "2720/2720 [==============================] - 2s 589us/step - loss: 0.1008 - accuracy: 0.9599 - val_loss: 0.2565 - val_accuracy: 0.9142\n",
            "Epoch 160/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.0975 - accuracy: 0.9592 - val_loss: 0.2794 - val_accuracy: 0.9010\n",
            "Epoch 161/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.1006 - accuracy: 0.9610 - val_loss: 0.2643 - val_accuracy: 0.9010\n",
            "Epoch 162/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.0944 - accuracy: 0.9654 - val_loss: 0.2720 - val_accuracy: 0.9043\n",
            "Epoch 163/200\n",
            "2720/2720 [==============================] - 2s 571us/step - loss: 0.0877 - accuracy: 0.9651 - val_loss: 0.2926 - val_accuracy: 0.8944\n",
            "Epoch 164/200\n",
            "2720/2720 [==============================] - 2s 569us/step - loss: 0.1089 - accuracy: 0.9581 - val_loss: 0.2806 - val_accuracy: 0.9043\n",
            "Epoch 165/200\n",
            "2720/2720 [==============================] - 2s 575us/step - loss: 0.0988 - accuracy: 0.9636 - val_loss: 0.2790 - val_accuracy: 0.8944\n",
            "Epoch 166/200\n",
            "2720/2720 [==============================] - 2s 574us/step - loss: 0.0808 - accuracy: 0.9680 - val_loss: 0.2975 - val_accuracy: 0.9010\n",
            "Epoch 167/200\n",
            "2720/2720 [==============================] - 2s 571us/step - loss: 0.0683 - accuracy: 0.9728 - val_loss: 0.2723 - val_accuracy: 0.9109\n",
            "Epoch 168/200\n",
            "2720/2720 [==============================] - 2s 560us/step - loss: 0.0721 - accuracy: 0.9732 - val_loss: 0.2800 - val_accuracy: 0.9076\n",
            "Epoch 169/200\n",
            "2720/2720 [==============================] - 2s 555us/step - loss: 0.0854 - accuracy: 0.9688 - val_loss: 0.2701 - val_accuracy: 0.9175\n",
            "Epoch 170/200\n",
            "2720/2720 [==============================] - 2s 559us/step - loss: 0.0841 - accuracy: 0.9746 - val_loss: 0.2526 - val_accuracy: 0.9109\n",
            "Epoch 171/200\n",
            "2720/2720 [==============================] - 2s 572us/step - loss: 0.0966 - accuracy: 0.9643 - val_loss: 0.2806 - val_accuracy: 0.9010\n",
            "Epoch 172/200\n",
            "2720/2720 [==============================] - 2s 571us/step - loss: 0.0853 - accuracy: 0.9691 - val_loss: 0.2683 - val_accuracy: 0.9109\n",
            "Epoch 173/200\n",
            "2720/2720 [==============================] - 2s 571us/step - loss: 0.0827 - accuracy: 0.9684 - val_loss: 0.2558 - val_accuracy: 0.9175\n",
            "Epoch 174/200\n",
            "2720/2720 [==============================] - 2s 572us/step - loss: 0.0835 - accuracy: 0.9662 - val_loss: 0.2540 - val_accuracy: 0.9175\n",
            "Epoch 175/200\n",
            "2720/2720 [==============================] - 2s 567us/step - loss: 0.0819 - accuracy: 0.9695 - val_loss: 0.2730 - val_accuracy: 0.9109\n",
            "Epoch 176/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.0895 - accuracy: 0.9665 - val_loss: 0.2624 - val_accuracy: 0.9043\n",
            "Epoch 177/200\n",
            "2720/2720 [==============================] - 2s 572us/step - loss: 0.0741 - accuracy: 0.9735 - val_loss: 0.2752 - val_accuracy: 0.9109\n",
            "Epoch 178/200\n",
            "2720/2720 [==============================] - 2s 569us/step - loss: 0.0754 - accuracy: 0.9699 - val_loss: 0.2717 - val_accuracy: 0.9142\n",
            "Epoch 179/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.0853 - accuracy: 0.9658 - val_loss: 0.2775 - val_accuracy: 0.9109\n",
            "Epoch 180/200\n",
            "2720/2720 [==============================] - 2s 595us/step - loss: 0.0870 - accuracy: 0.9658 - val_loss: 0.2904 - val_accuracy: 0.9076\n",
            "Epoch 181/200\n",
            "2720/2720 [==============================] - 2s 586us/step - loss: 0.0810 - accuracy: 0.9710 - val_loss: 0.3061 - val_accuracy: 0.8944\n",
            "Epoch 182/200\n",
            "2720/2720 [==============================] - 2s 597us/step - loss: 0.0848 - accuracy: 0.9676 - val_loss: 0.2855 - val_accuracy: 0.9076\n",
            "Epoch 183/200\n",
            "2720/2720 [==============================] - 2s 576us/step - loss: 0.0900 - accuracy: 0.9684 - val_loss: 0.2912 - val_accuracy: 0.9010\n",
            "Epoch 184/200\n",
            "2720/2720 [==============================] - 2s 591us/step - loss: 0.0750 - accuracy: 0.9735 - val_loss: 0.3109 - val_accuracy: 0.9010\n",
            "Epoch 185/200\n",
            "2720/2720 [==============================] - 2s 601us/step - loss: 0.0730 - accuracy: 0.9717 - val_loss: 0.3102 - val_accuracy: 0.9010\n",
            "Epoch 186/200\n",
            "2720/2720 [==============================] - 2s 575us/step - loss: 0.0793 - accuracy: 0.9717 - val_loss: 0.2694 - val_accuracy: 0.9043\n",
            "Epoch 187/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.0748 - accuracy: 0.9717 - val_loss: 0.2817 - val_accuracy: 0.9076\n",
            "Epoch 188/200\n",
            "2720/2720 [==============================] - 2s 552us/step - loss: 0.0779 - accuracy: 0.9695 - val_loss: 0.2809 - val_accuracy: 0.9109\n",
            "Epoch 189/200\n",
            "2720/2720 [==============================] - 2s 574us/step - loss: 0.0797 - accuracy: 0.9691 - val_loss: 0.2582 - val_accuracy: 0.9175\n",
            "Epoch 190/200\n",
            "2720/2720 [==============================] - 2s 570us/step - loss: 0.0715 - accuracy: 0.9768 - val_loss: 0.2873 - val_accuracy: 0.9175\n",
            "Epoch 191/200\n",
            "2720/2720 [==============================] - 2s 586us/step - loss: 0.0870 - accuracy: 0.9673 - val_loss: 0.2954 - val_accuracy: 0.9043\n",
            "Epoch 192/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.0754 - accuracy: 0.9735 - val_loss: 0.2859 - val_accuracy: 0.9076\n",
            "Epoch 193/200\n",
            "2720/2720 [==============================] - 2s 589us/step - loss: 0.0720 - accuracy: 0.9710 - val_loss: 0.2837 - val_accuracy: 0.9043\n",
            "Epoch 194/200\n",
            "2720/2720 [==============================] - 2s 567us/step - loss: 0.0722 - accuracy: 0.9710 - val_loss: 0.3308 - val_accuracy: 0.8977\n",
            "Epoch 195/200\n",
            "2720/2720 [==============================] - 2s 572us/step - loss: 0.0798 - accuracy: 0.9706 - val_loss: 0.3041 - val_accuracy: 0.9076\n",
            "Epoch 196/200\n",
            "2720/2720 [==============================] - 2s 557us/step - loss: 0.0687 - accuracy: 0.9721 - val_loss: 0.2849 - val_accuracy: 0.9109\n",
            "Epoch 197/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.0716 - accuracy: 0.9772 - val_loss: 0.2982 - val_accuracy: 0.9142\n",
            "Epoch 198/200\n",
            "2720/2720 [==============================] - 2s 575us/step - loss: 0.0699 - accuracy: 0.9724 - val_loss: 0.3084 - val_accuracy: 0.8944\n",
            "Epoch 199/200\n",
            "2720/2720 [==============================] - 2s 575us/step - loss: 0.0544 - accuracy: 0.9798 - val_loss: 0.3113 - val_accuracy: 0.9076\n",
            "Epoch 200/200\n",
            "2720/2720 [==============================] - 2s 570us/step - loss: 0.0663 - accuracy: 0.9754 - val_loss: 0.3066 - val_accuracy: 0.8977\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f8ee65229e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRNeeRvpiL89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00zybaPoLFhY",
        "colab_type": "text"
      },
      "source": [
        "# **K- Fold**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz37Ak9KLIGo",
        "colab_type": "code",
        "outputId": "96644753-7e52-4020-8ff2-5846d8942bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "yt = []\n",
        "yp = []\n",
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "for train_idx, val_idx in kf.split(X1):\n",
        "  X_train1 = X1[train_idx]\n",
        "  X_train2 = X2[train_idx]\n",
        "  X_train3 = X3[train_idx]\n",
        "  X_train4 = X4[train_idx]\n",
        "  y_train = y[train_idx]\n",
        "\n",
        "  X_val1 = X1[val_idx]\n",
        "  X_val2 = X2[val_idx]\n",
        "  X_val3 = X3[val_idx]\n",
        "  X_val4 = X4[val_idx]\n",
        "  y_val = y[val_idx]\n",
        "\n",
        "  inputA = Input(shape=(81,100)) # One\n",
        "  inputB = Input(shape=(80,100)) # two\n",
        "  inputC = Input(shape=(79,100)) # Three\n",
        "  inputD = Input(shape=(78,100)) # Four\n",
        "\n",
        "  a = Convolution1D(128, kernel_size=3, activation='relu', padding='same') (inputA)\n",
        "  a = Dropout(0.5) (a)\n",
        "  a = Convolution1D(64, kernel_size=3, activation='relu', padding='same') (a)\n",
        "  a = Dropout(0.5) (a)\n",
        "  a = Convolution1D(32, kernel_size=3, activation='relu', padding='same') (a)\n",
        "  a = Dropout(0.5) (a)\n",
        "  a = Flatten() (a)\n",
        "\n",
        "  b = Convolution1D(filters = 128, kernel_size= 3, activation='relu', padding='same') (inputB)\n",
        "  b = Dropout(0.50) (b)\n",
        "  b = Convolution1D(filters = 64, kernel_size = 3, activation='relu', padding='same') (b)\n",
        "  b = Dropout(0.50) (b)\n",
        "  b = Convolution1D(filters = 32, kernel_size = 3, activation='relu', padding='same') (b)\n",
        "  b = Dropout(0.50) (b)\n",
        "  b = Flatten() (b)\n",
        "\n",
        "  c = Convolution1D(256, kernel_size=3, activation='relu', padding='same') (inputC)\n",
        "  c = Dropout(0.5) (c)\n",
        "  c = Conv1D(128, kernel_size=3, activation='relu', padding='same') (c)\n",
        "  c = Dropout(0.5) (c)\n",
        "  c = Conv1D(64, kernel_size=3, activation='relu', padding='same') (c)\n",
        "  c = Dropout(0.5) (c)\n",
        "  c = Conv1D(32, kernel_size=3, activation='relu', padding='same') (c)\n",
        "  c = Dropout(0.5) (c)\n",
        "  c = Flatten() (c)\n",
        "\n",
        "  d = Convolution1D(128, kernel_size=3, activation='relu', padding='same') (inputD)\n",
        "  d = Dropout(0.5) (d)\n",
        "  d = Conv1D(64, kernel_size=3, activation='relu', padding='same') (d)\n",
        "  d = Dropout(0.5) (d)\n",
        "  d = Conv1D(32, kernel_size=3, activation='relu', padding='same') (d)\n",
        "  d = Dropout(0.5) (d)\n",
        "  d = Flatten() (d)\n",
        "\n",
        "  combined = concatenate([a, b, c, d])\n",
        "\n",
        "  h = Dropout(0.5) (combined)\n",
        "  h = Dense(128, activation='relu') (h)\n",
        "  h = Dropout(0.5) (h)\n",
        "  h = Dense(64, activation='relu') (h)\n",
        "  output = Dense(1, activation='sigmoid') (h)\n",
        "\n",
        "  model = Model(inputs=[inputA, inputB, inputC, inputD], outputs=output)\n",
        "  model.compile(Adam(lr=0.0003), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  model.fit([X_train1, X_train2, X_train3, X_train4], y_train, \n",
        "            validation_data=([X_val1, X_val2, X_val3, X_val4], y_val),\n",
        "            epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "\n",
        "  probabilities = model.predict([X_val1, X_val2, X_val3, X_val4])\n",
        "  predicted_classes = probabilities >= 0.5\n",
        "  predicted_classes = predicted_classes.astype(int)\n",
        "  yp.append(predicted_classes)\n",
        "  yt.append(y_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4576 samples, validate on 1144 samples\n",
            "Epoch 1/10\n",
            "4576/4576 [==============================] - 3s 713us/step - loss: 0.7719 - accuracy: 0.4952 - val_loss: 0.6884 - val_accuracy: 0.6110\n",
            "Epoch 2/10\n",
            "4576/4576 [==============================] - 2s 509us/step - loss: 0.6825 - accuracy: 0.5610 - val_loss: 0.5767 - val_accuracy: 0.7351\n",
            "Epoch 3/10\n",
            "4576/4576 [==============================] - 2s 503us/step - loss: 0.5564 - accuracy: 0.7310 - val_loss: 0.4884 - val_accuracy: 0.7736\n",
            "Epoch 4/10\n",
            "4576/4576 [==============================] - 2s 506us/step - loss: 0.5256 - accuracy: 0.7476 - val_loss: 0.4966 - val_accuracy: 0.7517\n",
            "Epoch 5/10\n",
            "4576/4576 [==============================] - 2s 502us/step - loss: 0.5211 - accuracy: 0.7509 - val_loss: 0.4778 - val_accuracy: 0.7841\n",
            "Epoch 6/10\n",
            "4576/4576 [==============================] - 2s 519us/step - loss: 0.4975 - accuracy: 0.7677 - val_loss: 0.4696 - val_accuracy: 0.7876\n",
            "Epoch 7/10\n",
            "4576/4576 [==============================] - 2s 506us/step - loss: 0.4915 - accuracy: 0.7743 - val_loss: 0.4689 - val_accuracy: 0.7937\n",
            "Epoch 8/10\n",
            "4576/4576 [==============================] - 2s 511us/step - loss: 0.4904 - accuracy: 0.7775 - val_loss: 0.4686 - val_accuracy: 0.7622\n",
            "Epoch 9/10\n",
            "4576/4576 [==============================] - 2s 513us/step - loss: 0.4691 - accuracy: 0.7832 - val_loss: 0.4509 - val_accuracy: 0.7998\n",
            "Epoch 10/10\n",
            "4576/4576 [==============================] - 2s 516us/step - loss: 0.4703 - accuracy: 0.7880 - val_loss: 0.4524 - val_accuracy: 0.7876\n",
            "Train on 4576 samples, validate on 1144 samples\n",
            "Epoch 1/10\n",
            "4576/4576 [==============================] - 4s 772us/step - loss: 0.7670 - accuracy: 0.5031 - val_loss: 0.6859 - val_accuracy: 0.6302\n",
            "Epoch 2/10\n",
            "4576/4576 [==============================] - 2s 524us/step - loss: 0.6495 - accuracy: 0.6075 - val_loss: 0.5387 - val_accuracy: 0.7535\n",
            "Epoch 3/10\n",
            "4576/4576 [==============================] - 2s 511us/step - loss: 0.5493 - accuracy: 0.7301 - val_loss: 0.4950 - val_accuracy: 0.7815\n",
            "Epoch 4/10\n",
            "4576/4576 [==============================] - 2s 524us/step - loss: 0.5195 - accuracy: 0.7570 - val_loss: 0.5196 - val_accuracy: 0.7561\n",
            "Epoch 5/10\n",
            "4576/4576 [==============================] - 2s 545us/step - loss: 0.5155 - accuracy: 0.7607 - val_loss: 0.4787 - val_accuracy: 0.7841\n",
            "Epoch 6/10\n",
            "4576/4576 [==============================] - 2s 541us/step - loss: 0.5032 - accuracy: 0.7681 - val_loss: 0.4746 - val_accuracy: 0.7850\n",
            "Epoch 7/10\n",
            "4576/4576 [==============================] - 3s 549us/step - loss: 0.4971 - accuracy: 0.7697 - val_loss: 0.4678 - val_accuracy: 0.7876\n",
            "Epoch 8/10\n",
            "4576/4576 [==============================] - 2s 532us/step - loss: 0.4803 - accuracy: 0.7861 - val_loss: 0.4870 - val_accuracy: 0.7727\n",
            "Epoch 9/10\n",
            "4576/4576 [==============================] - 2s 523us/step - loss: 0.4823 - accuracy: 0.7819 - val_loss: 0.4597 - val_accuracy: 0.7928\n",
            "Epoch 10/10\n",
            "4576/4576 [==============================] - 2s 519us/step - loss: 0.4740 - accuracy: 0.7869 - val_loss: 0.4509 - val_accuracy: 0.8016\n",
            "Train on 4576 samples, validate on 1144 samples\n",
            "Epoch 1/10\n",
            "4576/4576 [==============================] - 3s 741us/step - loss: 0.7468 - accuracy: 0.5009 - val_loss: 0.6913 - val_accuracy: 0.5437\n",
            "Epoch 2/10\n",
            "4576/4576 [==============================] - 2s 522us/step - loss: 0.6946 - accuracy: 0.5350 - val_loss: 0.6266 - val_accuracy: 0.6233\n",
            "Epoch 3/10\n",
            "4576/4576 [==============================] - 2s 512us/step - loss: 0.5748 - accuracy: 0.7045 - val_loss: 0.5569 - val_accuracy: 0.7448\n",
            "Epoch 4/10\n",
            "4576/4576 [==============================] - 2s 520us/step - loss: 0.5398 - accuracy: 0.7450 - val_loss: 0.4837 - val_accuracy: 0.7893\n",
            "Epoch 5/10\n",
            "4576/4576 [==============================] - 2s 510us/step - loss: 0.5111 - accuracy: 0.7590 - val_loss: 0.4693 - val_accuracy: 0.7893\n",
            "Epoch 6/10\n",
            "4576/4576 [==============================] - 2s 516us/step - loss: 0.4967 - accuracy: 0.7677 - val_loss: 0.4718 - val_accuracy: 0.7832\n",
            "Epoch 7/10\n",
            "4576/4576 [==============================] - 2s 516us/step - loss: 0.4888 - accuracy: 0.7721 - val_loss: 0.4575 - val_accuracy: 0.7823\n",
            "Epoch 8/10\n",
            "4576/4576 [==============================] - 2s 519us/step - loss: 0.4868 - accuracy: 0.7666 - val_loss: 0.4790 - val_accuracy: 0.7753\n",
            "Epoch 9/10\n",
            "4576/4576 [==============================] - 2s 513us/step - loss: 0.4870 - accuracy: 0.7747 - val_loss: 0.4610 - val_accuracy: 0.7745\n",
            "Epoch 10/10\n",
            "4576/4576 [==============================] - 2s 517us/step - loss: 0.4748 - accuracy: 0.7793 - val_loss: 0.4487 - val_accuracy: 0.7937\n",
            "Train on 4576 samples, validate on 1144 samples\n",
            "Epoch 1/10\n",
            "4576/4576 [==============================] - 3s 725us/step - loss: 0.7704 - accuracy: 0.5066 - val_loss: 0.6952 - val_accuracy: 0.4930\n",
            "Epoch 2/10\n",
            "4576/4576 [==============================] - 2s 500us/step - loss: 0.7074 - accuracy: 0.5122 - val_loss: 0.6863 - val_accuracy: 0.6477\n",
            "Epoch 3/10\n",
            "4576/4576 [==============================] - 2s 509us/step - loss: 0.6684 - accuracy: 0.5756 - val_loss: 0.5440 - val_accuracy: 0.7491\n",
            "Epoch 4/10\n",
            "4576/4576 [==============================] - 2s 517us/step - loss: 0.5586 - accuracy: 0.7275 - val_loss: 0.5110 - val_accuracy: 0.7622\n",
            "Epoch 5/10\n",
            "4576/4576 [==============================] - 2s 513us/step - loss: 0.5123 - accuracy: 0.7611 - val_loss: 0.5568 - val_accuracy: 0.7544\n",
            "Epoch 6/10\n",
            "4576/4576 [==============================] - 2s 518us/step - loss: 0.5170 - accuracy: 0.7635 - val_loss: 0.4907 - val_accuracy: 0.7771\n",
            "Epoch 7/10\n",
            "4576/4576 [==============================] - 2s 519us/step - loss: 0.5043 - accuracy: 0.7692 - val_loss: 0.4739 - val_accuracy: 0.7815\n",
            "Epoch 8/10\n",
            "4576/4576 [==============================] - 2s 519us/step - loss: 0.4875 - accuracy: 0.7762 - val_loss: 0.4749 - val_accuracy: 0.7841\n",
            "Epoch 9/10\n",
            "4576/4576 [==============================] - 2s 518us/step - loss: 0.4900 - accuracy: 0.7769 - val_loss: 0.4844 - val_accuracy: 0.7850\n",
            "Epoch 10/10\n",
            "4576/4576 [==============================] - 2s 510us/step - loss: 0.4754 - accuracy: 0.7815 - val_loss: 0.4420 - val_accuracy: 0.7972\n",
            "Train on 4576 samples, validate on 1144 samples\n",
            "Epoch 1/10\n",
            "4576/4576 [==============================] - 3s 714us/step - loss: 0.7641 - accuracy: 0.5133 - val_loss: 0.6858 - val_accuracy: 0.6049\n",
            "Epoch 2/10\n",
            "4576/4576 [==============================] - 2s 501us/step - loss: 0.6977 - accuracy: 0.5413 - val_loss: 0.6477 - val_accuracy: 0.5245\n",
            "Epoch 3/10\n",
            "4576/4576 [==============================] - 2s 510us/step - loss: 0.5969 - accuracy: 0.6807 - val_loss: 0.4798 - val_accuracy: 0.7937\n",
            "Epoch 4/10\n",
            "4576/4576 [==============================] - 2s 494us/step - loss: 0.5371 - accuracy: 0.7448 - val_loss: 0.4532 - val_accuracy: 0.7946\n",
            "Epoch 5/10\n",
            "4576/4576 [==============================] - 2s 504us/step - loss: 0.5179 - accuracy: 0.7574 - val_loss: 0.4629 - val_accuracy: 0.7841\n",
            "Epoch 6/10\n",
            "4576/4576 [==============================] - 2s 513us/step - loss: 0.5079 - accuracy: 0.7651 - val_loss: 0.4383 - val_accuracy: 0.8016\n",
            "Epoch 7/10\n",
            "4576/4576 [==============================] - 2s 514us/step - loss: 0.4950 - accuracy: 0.7745 - val_loss: 0.4237 - val_accuracy: 0.8129\n",
            "Epoch 8/10\n",
            "4576/4576 [==============================] - 2s 503us/step - loss: 0.4885 - accuracy: 0.7791 - val_loss: 0.4174 - val_accuracy: 0.8243\n",
            "Epoch 9/10\n",
            "4576/4576 [==============================] - 2s 503us/step - loss: 0.4744 - accuracy: 0.7819 - val_loss: 0.4215 - val_accuracy: 0.8191\n",
            "Epoch 10/10\n",
            "4576/4576 [==============================] - 2s 495us/step - loss: 0.4687 - accuracy: 0.7891 - val_loss: 0.4209 - val_accuracy: 0.8173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM-P5_u-LK_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true = []\n",
        "pred = []\n",
        "a=0\n",
        "for i in yt:\n",
        "  b=0\n",
        "  for j in i:\n",
        "    true.append(j)\n",
        "    pred.append(yp[a][b])\n",
        "    b=+1\n",
        "  a+=1\n",
        "    \n",
        "y_true = np.asarray(true)\n",
        "y_pred = np.asarray(pred)\n",
        "\n",
        "print('Acc: ', accuracy_score(y_true, y_pred) * 100)\n",
        "print('MCC: ', matthews_corrcoef(y_true, y_pred) * 100)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print('Sn:  ', (cm[1,1] / (cm[1,1] + cm[1,0])) * 100)\n",
        "print('Sp:  ', (cm[0,0] / (cm[0,0] + cm[0,1])) * 100)\n",
        "\n",
        "p = precision_recall_fscore_support(y_true, y_pred)\n",
        "print('pre_ 0: ', p[0][0] * 100)\n",
        "print('pre_ 1: ', p[0][1] * 100)\n",
        "print('Pre_weighted: ', precision_score(y_true, y_pred, average='weighted') * 100)\n",
        "\n",
        "print('F-score: ', np.mean(p[2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4Ed2SpTRPWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}