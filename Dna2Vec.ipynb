{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dna2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QrqH2P22tGWL",
        "9Y-1UrMqtJ0p",
        "mgydp5IDtSiD",
        "8LuY8yLadUvm",
        "TZ4QHLsYfth2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/habibsifat/Dna2Vec/blob/master/Dna2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmJ-BBjzQmem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4QKDxa_ODV4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ed77a618-e02f-4599-997a-2481e2b6c490"
      },
      "source": [
        "from __future__ import print_function\n",
        "__author__ = 'maxim'\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "import string\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.utils.data_utils import get_file\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5IOYY2dHCMm",
        "colab_type": "code",
        "outputId": "9c7b27c1-d2b6-4843-bf7f-2629fc194503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "corpus_file = '/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70ds44.txt'\n",
        "print('\\nPreparing the  sentences...')\n",
        "sentences = []\n",
        "with open(corpus_file, 'r', encoding=\"utf-8-sig\") as f:\n",
        "    for line in f:\n",
        "        sentences.append(line.split())\n",
        "        \n",
        "print('Num sentences:', len(sentences))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Preparing the  sentences...\n",
            "Num sentences: 3023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ95trPAtAk2",
        "colab_type": "text"
      },
      "source": [
        "# **w2v**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2Q3vhIVNb8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QC1fd3PNX40",
        "colab_type": "code",
        "outputId": "6f70ffa0-0a58-46e6-8c78-0673d3d4471c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "print('\\nTraining word2vec...')\n",
        "word_model = Word2Vec(sentences, size=100, min_count=1, window=5, iter=10)\n",
        "#pretrained_weights = word_model.wv.syn0 \n",
        "#vocab_size, emdedding_size = pretrained_weights.shape\n",
        "#print('Result embedding shape:', pretrained_weights.shape)\n",
        "len(word_model.wv.vocab)\n",
        "#Save partly trained model\n",
        "#word_model.save('w2v_model.h5')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training word2vec...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "261"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZil4xClLDLN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "0b95cb4c-6818-4840-c70d-75fd1b5c841f"
      },
      "source": [
        "word_model.wv.vocab"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A': <gensim.models.keyedvectors.Vocab at 0x7f8fbbe9e978>,\n",
              " 'C': <gensim.models.keyedvectors.Vocab at 0x7f8fbbe9e908>,\n",
              " 'G': <gensim.models.keyedvectors.Vocab at 0x7f8fa5c68e48>,\n",
              " 'T': <gensim.models.keyedvectors.Vocab at 0x7f8f5bd8d358>,\n",
              " '\\ufeff': <gensim.models.keyedvectors.Vocab at 0x7f8f5bd8d2b0>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NbNY9qiRSpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_model.wv['\\ufeff']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbEpgfnnmcBx",
        "colab_type": "text"
      },
      "source": [
        "# **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATjrZ_uMNsPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_weights = word_model.wv.syn0 \n",
        "vocab_size, emdedding_size = pretrained_weights.shape\n",
        "print('Result embedding shape:', pretrained_weights.shape)\n",
        "print(vocab_size)\n",
        "print(emdedding_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5ijHh5JNsvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2idx(word):\n",
        "  return word_model.wv.vocab[word].index\n",
        "def idx2word(idx):\n",
        "  return word_model.wv.index2word[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndbf70tHN7xK",
        "colab_type": "code",
        "outputId": "913bff40-7ddd-4995-f193-62ab1dbfffaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "max_sentence_len = 78\n",
        "print('\\nPreparing the data for LSTM...')\n",
        "x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
        "#y = np.zeros([len(sentences)], dtype=np.int32)\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, word in enumerate(sentence[:-1]):\n",
        "    x[i, t] = word2idx(word)\n",
        "    #y[i] = word2idx(sentence[-1])\n",
        "print('x shape:', x.shape)\n",
        "#print('y shape:', y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Preparing the data for LSTM...\n",
            "x shape: (5720, 78)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbqHaYalOAIw",
        "colab_type": "code",
        "outputId": "d8d8a17a-d9ba-46b5-9d2b-e8dbf9f9aceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y = np.array([1]*2860 + [0]*2860)\n",
        "print('y shape:', y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y shape: (5720,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK8Im-QQOKd5",
        "colab_type": "code",
        "outputId": "48d338b1-04a0-475c-fdb5-00650f49596f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "#from sklearn.cross_validation import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.1,random_state=10)\n",
        "#shape of train and test objects\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "# shape of new y objects\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5148, 78)\n",
            "(572, 78)\n",
            "(5148,)\n",
            "(572,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrqH2P22tGWL",
        "colab_type": "text"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMPF8GREOLUb",
        "colab_type": "code",
        "outputId": "71597781-6315-4f15-905c-3df3b6127dff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from keras import optimizers\n",
        "print('\\nTraining LSTM...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
        "model.add(LSTM(units=512,return_sequences=True))\n",
        "#model.add(Activation('tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(units=512, return_sequences=False))\n",
        "#model.add(Activation('tanh'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=vocab_size))\n",
        "model.add(Activation('softmax'))\n",
        "optz = optimizers.Adam(lr=1, clipnorm=5,beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model.compile(optimizer=optz, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training LSTM...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBbBSIaoORyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, Y_train,batch_size=128,epochs=5,shuffle=True, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y-1UrMqtJ0p",
        "colab_type": "text"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8OIrBiyPxmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Convolution1D, Dense, Dropout, Flatten\n",
        "from keras.optimizers import Adam,RMSprop\n",
        "from keras.callbacks import EarlyStopping\n",
        "model = Sequential()\n",
        "#model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
        "#model.add(Embedding(4, 100, input_length=81))\n",
        "model.add(Convolution1D(200, kernel_size=3, activation='relu', input_shape=(81, 1), name='m1_conv1'))\n",
        "#model.add(Dropout(0.5, name='m1_drop1'))\n",
        "model.add(Convolution1D(128, kernel_size=3, activation='relu', name='m1_conv2'))\n",
        "#model.add(Dropout(0.5, name='m1_drop2'))\n",
        "model.add(Convolution1D(64, kernel_size=3, activation='relu', name='m1_conv3'))\n",
        "#model.add(Dropout(0.5, name='m1_drop3'))\n",
        "model.add(Flatten(name='m1_flatten1'))\n",
        "#model.add(Dropout(0.5, name='m1_drop4'))\n",
        "model.add(Dense(64, activation='relu', name='m1_dense1'))\n",
        "model.add(Dense(1, activation='softmax', name='m1_dense2'))\n",
        "\n",
        "#model.compile(RMSprop(lr=.003), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(Adam(lr=.003), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', patience=16)\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=5, batch_size=32, callbacks=[es])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcsZEWNAtPSk",
        "colab_type": "text"
      },
      "source": [
        "# 1-Mer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAkPNcNut9WP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/promoter70.txt')\n",
        "seqs = df.iloc[:, 0].tolist()\n",
        "\n",
        "for i in range(len(seqs)):\n",
        "  seqs[i] = seqs[i].upper()\n",
        "  \n",
        "with open('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70ds11.txt', 'w', encoding=\"utf-8-sig\") as f:\n",
        "  for j in seqs:\n",
        "    for k in j:\n",
        "      f.write(k)\n",
        "      f.write(\" \")\n",
        "    f.write(\"\\n\")\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvLcDjD3sxbH",
        "colab_type": "code",
        "outputId": "b2bd6599-0ab3-4130-d8a1-8a1a27851e19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNnaRy4xv-sS",
        "colab_type": "text"
      },
      "source": [
        "**this cell**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nWoq8YqTtCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A4 = np.zeros([2860,81,100],dtype=np.int32)\n",
        "\n",
        "for i in range(0,len(sentences)): \n",
        "  p = 0\n",
        "  for j in sentences[i]:\n",
        "    q = 0\n",
        "    for k in word_model.wv[j]:\n",
        "      r = 0\n",
        "      A4[p][q][r] = k\n",
        "      r=r+1\n",
        "    q=q+1\n",
        "  p=p+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL21eaWl4K7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#second dimesion of the array 81 because there are total 81 1-mers\n",
        "A2 = np.zeros([3023,79,100])\n",
        "\n",
        "for current_sequence_number in range(0,len(sentences)):\n",
        "\n",
        "  kmer_counter = 0\n",
        "  current_sequence = sentences[current_sequence_number] \n",
        "  \n",
        "  #here j contains a k-mer from current_sequence\n",
        "\n",
        "  for kmer in current_sequence:\n",
        "    \n",
        "    A2[current_sequence_number,kmer_counter,:] = word_model.wv[kmer] \n",
        "\n",
        "    kmer_counter = kmer_counter + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9dSLTQ5ckst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70A44.npy',A2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GawrGwYub0Ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A2[1][1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b6mzbMP4rPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_model.wv['CGCG']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uga9madSb3Qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences[1][1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgydp5IDtSiD",
        "colab_type": "text"
      },
      "source": [
        "# 2-Mer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiqRFi_AcAgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/promoter70.txt')\n",
        "seqs = df.iloc[:, 0].tolist()\n",
        "\n",
        "for i in range(len(seqs)):\n",
        "  seqs[i] = seqs[i].upper()\n",
        "  \n",
        "with open('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70ds22.txt', 'w', encoding=\"utf-8-sig\") as f:#\n",
        "  for j in seqs:\n",
        "    for k in range(0,len(j)-1):\n",
        "      #print(j[k],end='')\n",
        "      #print(j[k+1],end=' ')\n",
        "      f.write(j[k])\n",
        "      f.write(j[k+1])\n",
        "      f.write(\" \")\n",
        "    f.write(\"\\n\")\n",
        "    #print()\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBIwrHZH409o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LuY8yLadUvm",
        "colab_type": "text"
      },
      "source": [
        "# **3-Mer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q9TAJvsdYkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/promoter70.txt')\n",
        "seqs = df.iloc[:, 0].tolist()\n",
        "\n",
        "for i in range(len(seqs)):\n",
        "  seqs[i] = seqs[i].upper()\n",
        "  \n",
        "with open('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70ds33.txt', 'w', encoding=\"utf-8-sig\") as f:#\n",
        "  for j in seqs:\n",
        "    for k in range(0,len(j)-2):\n",
        "      #print(j[k],end='')\n",
        "      #print(j[k+1],end=' ')\n",
        "      f.write(j[k])\n",
        "      f.write(j[k+1])\n",
        "      f.write(j[k+2])\n",
        "      f.write(\" \")\n",
        "    f.write(\"\\n\")\n",
        "    #print()\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ4QHLsYfth2",
        "colab_type": "text"
      },
      "source": [
        "# **4-Mer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY8LsQikfxOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/promoter70.txt')\n",
        "seqs = df.iloc[:, 0].tolist()\n",
        "\n",
        "for i in range(len(seqs)):\n",
        "  seqs[i] = seqs[i].upper()\n",
        "  \n",
        "with open('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70ds44.txt', 'w', encoding=\"utf-8-sig\") as f:#\n",
        "  for j in seqs:\n",
        "    for k in range(0,len(j)-3):\n",
        "      #print(j[k],end='')\n",
        "      #print(j[k+1],end=' ')\n",
        "      f.write(j[k])\n",
        "      f.write(j[k+1])\n",
        "      f.write(j[k+2])\n",
        "      f.write(j[k+3])\n",
        "      f.write(\" \")\n",
        "    f.write(\"\\n\")\n",
        "    #print()\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE4VYcEKhYhg",
        "colab_type": "text"
      },
      "source": [
        "# **MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4vLEkfJ8g64",
        "colab_type": "code",
        "outputId": "a132d34a-ec58-403e-cbab-84bd030d98e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "X1 = np.load('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70A11.npy')\n",
        "X2 = np.load('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70A22.npy')\n",
        "X3 = np.load('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70A33.npy')\n",
        "X4 = np.load('/content/drive/My Drive/Colab Notebooks/BIO/Promoter/Dna2Vec/Sigma70/70A44.npy')\n",
        "print('X shape:', X1.shape)\n",
        "y = np.array([1]*1694 + [0]*1329)\n",
        "print('y shape:', y.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape: (3023, 82, 100)\n",
            "y shape: (3023,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dPAOYHw9QLA",
        "colab_type": "code",
        "outputId": "4383be3d-aaa9-4bb9-be65-9e93dfb28651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train1,X_test1,Y_train,Y_test = train_test_split(X1,y,test_size=0.1,random_state=10)\n",
        "X_train2,X_test2,Y_train,Y_test = train_test_split(X2,y,test_size=0.1,random_state=10)\n",
        "X_train3,X_test3,Y_train,Y_test = train_test_split(X3,y,test_size=0.1,random_state=10)\n",
        "X_train4,X_test4,Y_train,Y_test = train_test_split(X4,y,test_size=0.1,random_state=10)\n",
        "#shape of train and test objects\n",
        "print(X_train1.shape)\n",
        "print(X_test1.shape)\n",
        "# shape of new y objects\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2720, 82, 100)\n",
            "(303, 82, 100)\n",
            "(2720,)\n",
            "(303,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeDf_EpHhcNc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21d82b1e-56d7-4f8d-fc25-ba48fe431da5"
      },
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Convolution1D, Dense, Dropout, Flatten, Conv1D, concatenate\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "inputA = Input(shape=(82,100)) # One\n",
        "inputB = Input(shape=(81,100)) # two\n",
        "inputC = Input(shape=(80,100)) # Three\n",
        "inputD = Input(shape=(79,100)) # Four\n",
        "\n",
        "a = Convolution1D(128, kernel_size=3, activation='relu', padding='same') (inputA)\n",
        "a = Dropout(0.5) (a)\n",
        "a = Convolution1D(64, kernel_size=3, activation='relu', padding='same') (a)\n",
        "a = Dropout(0.5) (a)\n",
        "a = Convolution1D(32, kernel_size=3, activation='relu', padding='same') (a)\n",
        "a = Dropout(0.5) (a)\n",
        "a = Flatten() (a)\n",
        "\n",
        "b = Convolution1D(filters = 128, kernel_size= 3, activation='relu', padding='same') (inputB)\n",
        "b = Dropout(0.50) (b)\n",
        "b = Convolution1D(filters = 64, kernel_size = 3, activation='relu', padding='same') (b)\n",
        "b = Dropout(0.50) (b)\n",
        "b = Convolution1D(filters = 32, kernel_size = 3, activation='relu', padding='same') (b)\n",
        "b = Dropout(0.50) (b)\n",
        "b = Flatten() (b)\n",
        "\n",
        "c = Convolution1D(256, kernel_size=3, activation='relu', padding='same') (inputC)\n",
        "c = Dropout(0.5) (c)\n",
        "c = Conv1D(128, kernel_size=3, activation='relu', padding='same') (c)\n",
        "c = Dropout(0.5) (c)\n",
        "c = Conv1D(64, kernel_size=3, activation='relu', padding='same') (c)\n",
        "c = Dropout(0.5) (c)\n",
        "c = Conv1D(32, kernel_size=3, activation='relu', padding='same') (c)\n",
        "c = Dropout(0.5) (c)\n",
        "c = Flatten() (c)\n",
        "\n",
        "d = Convolution1D(128, kernel_size=3, activation='relu', padding='same') (inputD)\n",
        "d = Dropout(0.5) (d)\n",
        "d = Conv1D(64, kernel_size=3, activation='relu', padding='same') (d)\n",
        "d = Dropout(0.5) (d)\n",
        "d = Conv1D(32, kernel_size=3, activation='relu', padding='same') (d)\n",
        "d = Dropout(0.5) (d)\n",
        "d = Flatten() (d)\n",
        "\n",
        "combined = concatenate([a, b, c, d])\n",
        "\n",
        "h = Dropout(0.5) (combined)\n",
        "h = Dense(128, activation='relu') (h)\n",
        "h = Dropout(0.5) (h)\n",
        "h = Dense(64, activation='relu') (h)\n",
        "output = Dense(1, activation='sigmoid') (h)\n",
        "\n",
        "model = Model(inputs=[inputA, inputB, inputC, inputD], outputs=output)\n",
        "model.compile(Adam(lr=0.0003), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit([X_train1, X_train2, X_train3, X_train4], Y_train, validation_data=([X_test1, X_test2, X_test3, X_test4], Y_test),\n",
        "          epochs=200, batch_size=32, verbose=1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2720 samples, validate on 303 samples\n",
            "Epoch 1/200\n",
            "2720/2720 [==============================] - 3s 986us/step - loss: 0.7873 - accuracy: 0.5232 - val_loss: 0.6953 - val_accuracy: 0.4686\n",
            "Epoch 2/200\n",
            "2720/2720 [==============================] - 2s 586us/step - loss: 0.7198 - accuracy: 0.5316 - val_loss: 0.6890 - val_accuracy: 0.5776\n",
            "Epoch 3/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.7040 - accuracy: 0.5199 - val_loss: 0.6890 - val_accuracy: 0.5809\n",
            "Epoch 4/200\n",
            "2720/2720 [==============================] - 2s 591us/step - loss: 0.6945 - accuracy: 0.5474 - val_loss: 0.6902 - val_accuracy: 0.5809\n",
            "Epoch 5/200\n",
            "2720/2720 [==============================] - 2s 608us/step - loss: 0.6904 - accuracy: 0.5456 - val_loss: 0.6746 - val_accuracy: 0.5974\n",
            "Epoch 6/200\n",
            "2720/2720 [==============================] - 2s 586us/step - loss: 0.6600 - accuracy: 0.6040 - val_loss: 0.6215 - val_accuracy: 0.6139\n",
            "Epoch 7/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.6383 - accuracy: 0.6368 - val_loss: 0.5854 - val_accuracy: 0.6997\n",
            "Epoch 8/200\n",
            "2720/2720 [==============================] - 2s 576us/step - loss: 0.6052 - accuracy: 0.6787 - val_loss: 0.5236 - val_accuracy: 0.7690\n",
            "Epoch 9/200\n",
            "2720/2720 [==============================] - 2s 576us/step - loss: 0.5892 - accuracy: 0.6875 - val_loss: 0.5223 - val_accuracy: 0.7558\n",
            "Epoch 10/200\n",
            "2720/2720 [==============================] - 2s 588us/step - loss: 0.5571 - accuracy: 0.7235 - val_loss: 0.5037 - val_accuracy: 0.7657\n",
            "Epoch 11/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.5523 - accuracy: 0.7169 - val_loss: 0.5043 - val_accuracy: 0.7921\n",
            "Epoch 12/200\n",
            "2720/2720 [==============================] - 2s 589us/step - loss: 0.5325 - accuracy: 0.7349 - val_loss: 0.4996 - val_accuracy: 0.7525\n",
            "Epoch 13/200\n",
            "2720/2720 [==============================] - 2s 586us/step - loss: 0.5175 - accuracy: 0.7460 - val_loss: 0.4738 - val_accuracy: 0.7987\n",
            "Epoch 14/200\n",
            "2720/2720 [==============================] - 2s 574us/step - loss: 0.5171 - accuracy: 0.7548 - val_loss: 0.4679 - val_accuracy: 0.8086\n",
            "Epoch 15/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.4974 - accuracy: 0.7665 - val_loss: 0.4494 - val_accuracy: 0.8020\n",
            "Epoch 16/200\n",
            "2720/2720 [==============================] - 2s 573us/step - loss: 0.5063 - accuracy: 0.7482 - val_loss: 0.4583 - val_accuracy: 0.7888\n",
            "Epoch 17/200\n",
            "2720/2720 [==============================] - 2s 570us/step - loss: 0.4721 - accuracy: 0.7794 - val_loss: 0.4368 - val_accuracy: 0.8152\n",
            "Epoch 18/200\n",
            "2720/2720 [==============================] - 2s 587us/step - loss: 0.4772 - accuracy: 0.7735 - val_loss: 0.4610 - val_accuracy: 0.7789\n",
            "Epoch 19/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.4697 - accuracy: 0.7809 - val_loss: 0.4385 - val_accuracy: 0.8086\n",
            "Epoch 20/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.4664 - accuracy: 0.7908 - val_loss: 0.4282 - val_accuracy: 0.8251\n",
            "Epoch 21/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.4645 - accuracy: 0.7897 - val_loss: 0.4191 - val_accuracy: 0.8218\n",
            "Epoch 22/200\n",
            "2720/2720 [==============================] - 2s 572us/step - loss: 0.4637 - accuracy: 0.7860 - val_loss: 0.4289 - val_accuracy: 0.8053\n",
            "Epoch 23/200\n",
            "2720/2720 [==============================] - 2s 590us/step - loss: 0.4378 - accuracy: 0.7956 - val_loss: 0.4262 - val_accuracy: 0.7987\n",
            "Epoch 24/200\n",
            "2720/2720 [==============================] - 2s 575us/step - loss: 0.4479 - accuracy: 0.7971 - val_loss: 0.4248 - val_accuracy: 0.8152\n",
            "Epoch 25/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.4575 - accuracy: 0.7875 - val_loss: 0.4326 - val_accuracy: 0.8086\n",
            "Epoch 26/200\n",
            "2720/2720 [==============================] - 2s 572us/step - loss: 0.4383 - accuracy: 0.8037 - val_loss: 0.4146 - val_accuracy: 0.8317\n",
            "Epoch 27/200\n",
            "2720/2720 [==============================] - 2s 591us/step - loss: 0.4219 - accuracy: 0.8085 - val_loss: 0.4026 - val_accuracy: 0.8152\n",
            "Epoch 28/200\n",
            "2720/2720 [==============================] - 2s 604us/step - loss: 0.4217 - accuracy: 0.8066 - val_loss: 0.4056 - val_accuracy: 0.8218\n",
            "Epoch 29/200\n",
            "2720/2720 [==============================] - 2s 597us/step - loss: 0.4246 - accuracy: 0.8125 - val_loss: 0.4061 - val_accuracy: 0.8383\n",
            "Epoch 30/200\n",
            "2720/2720 [==============================] - 2s 588us/step - loss: 0.4173 - accuracy: 0.8114 - val_loss: 0.3930 - val_accuracy: 0.8251\n",
            "Epoch 31/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.4100 - accuracy: 0.8184 - val_loss: 0.4010 - val_accuracy: 0.8383\n",
            "Epoch 32/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.4000 - accuracy: 0.8261 - val_loss: 0.3909 - val_accuracy: 0.8416\n",
            "Epoch 33/200\n",
            "2720/2720 [==============================] - 2s 569us/step - loss: 0.3955 - accuracy: 0.8324 - val_loss: 0.3857 - val_accuracy: 0.8284\n",
            "Epoch 34/200\n",
            "2720/2720 [==============================] - 2s 572us/step - loss: 0.3964 - accuracy: 0.8276 - val_loss: 0.4040 - val_accuracy: 0.8482\n",
            "Epoch 35/200\n",
            "2720/2720 [==============================] - 2s 576us/step - loss: 0.3994 - accuracy: 0.8232 - val_loss: 0.4004 - val_accuracy: 0.8449\n",
            "Epoch 36/200\n",
            "2720/2720 [==============================] - 2s 589us/step - loss: 0.3992 - accuracy: 0.8224 - val_loss: 0.3855 - val_accuracy: 0.8515\n",
            "Epoch 37/200\n",
            "2720/2720 [==============================] - 2s 589us/step - loss: 0.3808 - accuracy: 0.8335 - val_loss: 0.3748 - val_accuracy: 0.8482\n",
            "Epoch 38/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.3871 - accuracy: 0.8294 - val_loss: 0.3858 - val_accuracy: 0.8185\n",
            "Epoch 39/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.3885 - accuracy: 0.8360 - val_loss: 0.3803 - val_accuracy: 0.8449\n",
            "Epoch 40/200\n",
            "2720/2720 [==============================] - 2s 561us/step - loss: 0.3704 - accuracy: 0.8309 - val_loss: 0.3699 - val_accuracy: 0.8383\n",
            "Epoch 41/200\n",
            "2720/2720 [==============================] - 2s 574us/step - loss: 0.3663 - accuracy: 0.8393 - val_loss: 0.3783 - val_accuracy: 0.8515\n",
            "Epoch 42/200\n",
            "2720/2720 [==============================] - 2s 569us/step - loss: 0.3616 - accuracy: 0.8393 - val_loss: 0.3681 - val_accuracy: 0.8515\n",
            "Epoch 43/200\n",
            "2720/2720 [==============================] - 2s 569us/step - loss: 0.3516 - accuracy: 0.8412 - val_loss: 0.3630 - val_accuracy: 0.8482\n",
            "Epoch 44/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.3580 - accuracy: 0.8426 - val_loss: 0.3662 - val_accuracy: 0.8416\n",
            "Epoch 45/200\n",
            "2720/2720 [==============================] - 2s 576us/step - loss: 0.3420 - accuracy: 0.8522 - val_loss: 0.3647 - val_accuracy: 0.8515\n",
            "Epoch 46/200\n",
            "2720/2720 [==============================] - 2s 568us/step - loss: 0.3445 - accuracy: 0.8415 - val_loss: 0.3624 - val_accuracy: 0.8647\n",
            "Epoch 47/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.3339 - accuracy: 0.8526 - val_loss: 0.3615 - val_accuracy: 0.8581\n",
            "Epoch 48/200\n",
            "2720/2720 [==============================] - 2s 575us/step - loss: 0.3426 - accuracy: 0.8434 - val_loss: 0.3546 - val_accuracy: 0.8548\n",
            "Epoch 49/200\n",
            "2720/2720 [==============================] - 2s 595us/step - loss: 0.3534 - accuracy: 0.8434 - val_loss: 0.3518 - val_accuracy: 0.8647\n",
            "Epoch 50/200\n",
            "2720/2720 [==============================] - 2s 572us/step - loss: 0.3334 - accuracy: 0.8577 - val_loss: 0.3593 - val_accuracy: 0.8680\n",
            "Epoch 51/200\n",
            "2720/2720 [==============================] - 2s 587us/step - loss: 0.3345 - accuracy: 0.8548 - val_loss: 0.3446 - val_accuracy: 0.8713\n",
            "Epoch 52/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.3186 - accuracy: 0.8636 - val_loss: 0.3537 - val_accuracy: 0.8548\n",
            "Epoch 53/200\n",
            "2720/2720 [==============================] - 2s 575us/step - loss: 0.3282 - accuracy: 0.8607 - val_loss: 0.3583 - val_accuracy: 0.8581\n",
            "Epoch 54/200\n",
            "2720/2720 [==============================] - 2s 593us/step - loss: 0.3343 - accuracy: 0.8585 - val_loss: 0.3493 - val_accuracy: 0.8515\n",
            "Epoch 55/200\n",
            "2720/2720 [==============================] - 2s 586us/step - loss: 0.3127 - accuracy: 0.8680 - val_loss: 0.3460 - val_accuracy: 0.8548\n",
            "Epoch 56/200\n",
            "2720/2720 [==============================] - 2s 587us/step - loss: 0.3170 - accuracy: 0.8621 - val_loss: 0.3467 - val_accuracy: 0.8779\n",
            "Epoch 57/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.3114 - accuracy: 0.8610 - val_loss: 0.3393 - val_accuracy: 0.8581\n",
            "Epoch 58/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.3032 - accuracy: 0.8728 - val_loss: 0.3402 - val_accuracy: 0.8680\n",
            "Epoch 59/200\n",
            "2720/2720 [==============================] - 2s 575us/step - loss: 0.3105 - accuracy: 0.8647 - val_loss: 0.3520 - val_accuracy: 0.8779\n",
            "Epoch 60/200\n",
            "2720/2720 [==============================] - 2s 589us/step - loss: 0.2829 - accuracy: 0.8820 - val_loss: 0.3250 - val_accuracy: 0.8614\n",
            "Epoch 61/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.2877 - accuracy: 0.8787 - val_loss: 0.3224 - val_accuracy: 0.8746\n",
            "Epoch 62/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.2968 - accuracy: 0.8702 - val_loss: 0.3317 - val_accuracy: 0.8680\n",
            "Epoch 63/200\n",
            "2720/2720 [==============================] - 2s 576us/step - loss: 0.2923 - accuracy: 0.8750 - val_loss: 0.3298 - val_accuracy: 0.8647\n",
            "Epoch 64/200\n",
            "2720/2720 [==============================] - 2s 576us/step - loss: 0.2862 - accuracy: 0.8816 - val_loss: 0.3293 - val_accuracy: 0.8713\n",
            "Epoch 65/200\n",
            "2720/2720 [==============================] - 2s 574us/step - loss: 0.2931 - accuracy: 0.8768 - val_loss: 0.3314 - val_accuracy: 0.8680\n",
            "Epoch 66/200\n",
            "2720/2720 [==============================] - 2s 574us/step - loss: 0.2692 - accuracy: 0.8875 - val_loss: 0.3240 - val_accuracy: 0.8713\n",
            "Epoch 67/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.2739 - accuracy: 0.8805 - val_loss: 0.3181 - val_accuracy: 0.8680\n",
            "Epoch 68/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.2747 - accuracy: 0.8846 - val_loss: 0.3369 - val_accuracy: 0.8713\n",
            "Epoch 69/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.2726 - accuracy: 0.8871 - val_loss: 0.3266 - val_accuracy: 0.8647\n",
            "Epoch 70/200\n",
            "2720/2720 [==============================] - 2s 598us/step - loss: 0.2566 - accuracy: 0.8949 - val_loss: 0.3254 - val_accuracy: 0.8746\n",
            "Epoch 71/200\n",
            "2720/2720 [==============================] - 2s 609us/step - loss: 0.2663 - accuracy: 0.8846 - val_loss: 0.3167 - val_accuracy: 0.8713\n",
            "Epoch 72/200\n",
            "2720/2720 [==============================] - 2s 589us/step - loss: 0.2616 - accuracy: 0.8901 - val_loss: 0.3266 - val_accuracy: 0.8614\n",
            "Epoch 73/200\n",
            "2720/2720 [==============================] - 2s 589us/step - loss: 0.2665 - accuracy: 0.8890 - val_loss: 0.3153 - val_accuracy: 0.8845\n",
            "Epoch 74/200\n",
            "2720/2720 [==============================] - 2s 592us/step - loss: 0.2479 - accuracy: 0.8938 - val_loss: 0.3178 - val_accuracy: 0.8779\n",
            "Epoch 75/200\n",
            "2720/2720 [==============================] - 2s 596us/step - loss: 0.2547 - accuracy: 0.8923 - val_loss: 0.3245 - val_accuracy: 0.8713\n",
            "Epoch 76/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.2464 - accuracy: 0.8915 - val_loss: 0.3309 - val_accuracy: 0.8680\n",
            "Epoch 77/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.2496 - accuracy: 0.8963 - val_loss: 0.3264 - val_accuracy: 0.8746\n",
            "Epoch 78/200\n",
            "2720/2720 [==============================] - 2s 588us/step - loss: 0.2396 - accuracy: 0.9029 - val_loss: 0.3095 - val_accuracy: 0.8812\n",
            "Epoch 79/200\n",
            "2720/2720 [==============================] - 2s 584us/step - loss: 0.2371 - accuracy: 0.9033 - val_loss: 0.3038 - val_accuracy: 0.8779\n",
            "Epoch 80/200\n",
            "2720/2720 [==============================] - 2s 591us/step - loss: 0.2457 - accuracy: 0.9007 - val_loss: 0.2989 - val_accuracy: 0.8878\n",
            "Epoch 81/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.2445 - accuracy: 0.8974 - val_loss: 0.3016 - val_accuracy: 0.8746\n",
            "Epoch 82/200\n",
            "2720/2720 [==============================] - 2s 594us/step - loss: 0.2245 - accuracy: 0.9055 - val_loss: 0.3012 - val_accuracy: 0.8845\n",
            "Epoch 83/200\n",
            "2720/2720 [==============================] - 2s 584us/step - loss: 0.2263 - accuracy: 0.9040 - val_loss: 0.2926 - val_accuracy: 0.8746\n",
            "Epoch 84/200\n",
            "2720/2720 [==============================] - 2s 574us/step - loss: 0.2307 - accuracy: 0.9088 - val_loss: 0.3037 - val_accuracy: 0.8680\n",
            "Epoch 85/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.2185 - accuracy: 0.9044 - val_loss: 0.3037 - val_accuracy: 0.8713\n",
            "Epoch 86/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.2125 - accuracy: 0.9055 - val_loss: 0.2902 - val_accuracy: 0.8746\n",
            "Epoch 87/200\n",
            "2720/2720 [==============================] - 2s 590us/step - loss: 0.2348 - accuracy: 0.9037 - val_loss: 0.2939 - val_accuracy: 0.8812\n",
            "Epoch 88/200\n",
            "2720/2720 [==============================] - 2s 590us/step - loss: 0.2230 - accuracy: 0.9037 - val_loss: 0.2861 - val_accuracy: 0.8845\n",
            "Epoch 89/200\n",
            "2720/2720 [==============================] - 2s 573us/step - loss: 0.2157 - accuracy: 0.9085 - val_loss: 0.2978 - val_accuracy: 0.8812\n",
            "Epoch 90/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.2235 - accuracy: 0.9037 - val_loss: 0.2949 - val_accuracy: 0.8812\n",
            "Epoch 91/200\n",
            "2720/2720 [==============================] - 2s 586us/step - loss: 0.2146 - accuracy: 0.9147 - val_loss: 0.2954 - val_accuracy: 0.8944\n",
            "Epoch 92/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.1940 - accuracy: 0.9228 - val_loss: 0.2836 - val_accuracy: 0.8911\n",
            "Epoch 93/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.1974 - accuracy: 0.9268 - val_loss: 0.2773 - val_accuracy: 0.8746\n",
            "Epoch 94/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.2061 - accuracy: 0.9202 - val_loss: 0.2885 - val_accuracy: 0.8812\n",
            "Epoch 95/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.1996 - accuracy: 0.9176 - val_loss: 0.2855 - val_accuracy: 0.8779\n",
            "Epoch 96/200\n",
            "2720/2720 [==============================] - 2s 594us/step - loss: 0.1879 - accuracy: 0.9239 - val_loss: 0.2947 - val_accuracy: 0.8812\n",
            "Epoch 97/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.1933 - accuracy: 0.9165 - val_loss: 0.2787 - val_accuracy: 0.8779\n",
            "Epoch 98/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.1881 - accuracy: 0.9213 - val_loss: 0.2943 - val_accuracy: 0.8845\n",
            "Epoch 99/200\n",
            "2720/2720 [==============================] - 2s 593us/step - loss: 0.1980 - accuracy: 0.9169 - val_loss: 0.2890 - val_accuracy: 0.8713\n",
            "Epoch 100/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.1876 - accuracy: 0.9224 - val_loss: 0.2866 - val_accuracy: 0.8845\n",
            "Epoch 101/200\n",
            "2720/2720 [==============================] - 2s 592us/step - loss: 0.1974 - accuracy: 0.9129 - val_loss: 0.2901 - val_accuracy: 0.8713\n",
            "Epoch 102/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.1765 - accuracy: 0.9338 - val_loss: 0.2892 - val_accuracy: 0.8812\n",
            "Epoch 103/200\n",
            "2720/2720 [==============================] - 2s 591us/step - loss: 0.1759 - accuracy: 0.9298 - val_loss: 0.2804 - val_accuracy: 0.8779\n",
            "Epoch 104/200\n",
            "2720/2720 [==============================] - 2s 575us/step - loss: 0.1695 - accuracy: 0.9331 - val_loss: 0.2958 - val_accuracy: 0.8878\n",
            "Epoch 105/200\n",
            "2720/2720 [==============================] - 2s 588us/step - loss: 0.1809 - accuracy: 0.9283 - val_loss: 0.2710 - val_accuracy: 0.8845\n",
            "Epoch 106/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.1792 - accuracy: 0.9210 - val_loss: 0.2771 - val_accuracy: 0.8878\n",
            "Epoch 107/200\n",
            "2720/2720 [==============================] - 2s 588us/step - loss: 0.1780 - accuracy: 0.9331 - val_loss: 0.2780 - val_accuracy: 0.8845\n",
            "Epoch 108/200\n",
            "2720/2720 [==============================] - 2s 599us/step - loss: 0.1698 - accuracy: 0.9342 - val_loss: 0.3139 - val_accuracy: 0.8845\n",
            "Epoch 109/200\n",
            "2720/2720 [==============================] - 2s 575us/step - loss: 0.1764 - accuracy: 0.9316 - val_loss: 0.2901 - val_accuracy: 0.8911\n",
            "Epoch 110/200\n",
            "2720/2720 [==============================] - 2s 584us/step - loss: 0.1750 - accuracy: 0.9327 - val_loss: 0.2746 - val_accuracy: 0.8878\n",
            "Epoch 111/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.1752 - accuracy: 0.9287 - val_loss: 0.3166 - val_accuracy: 0.8845\n",
            "Epoch 112/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.1652 - accuracy: 0.9331 - val_loss: 0.2892 - val_accuracy: 0.8812\n",
            "Epoch 113/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.1473 - accuracy: 0.9426 - val_loss: 0.2909 - val_accuracy: 0.8878\n",
            "Epoch 114/200\n",
            "2720/2720 [==============================] - 2s 592us/step - loss: 0.1464 - accuracy: 0.9423 - val_loss: 0.2910 - val_accuracy: 0.8878\n",
            "Epoch 115/200\n",
            "2720/2720 [==============================] - 2s 597us/step - loss: 0.1604 - accuracy: 0.9342 - val_loss: 0.2944 - val_accuracy: 0.8845\n",
            "Epoch 116/200\n",
            "2720/2720 [==============================] - 2s 592us/step - loss: 0.1623 - accuracy: 0.9301 - val_loss: 0.3049 - val_accuracy: 0.8911\n",
            "Epoch 117/200\n",
            "2720/2720 [==============================] - 2s 606us/step - loss: 0.1508 - accuracy: 0.9364 - val_loss: 0.2944 - val_accuracy: 0.8779\n",
            "Epoch 118/200\n",
            "2720/2720 [==============================] - 2s 618us/step - loss: 0.1599 - accuracy: 0.9353 - val_loss: 0.2891 - val_accuracy: 0.8944\n",
            "Epoch 119/200\n",
            "2720/2720 [==============================] - 2s 615us/step - loss: 0.1500 - accuracy: 0.9412 - val_loss: 0.3024 - val_accuracy: 0.8911\n",
            "Epoch 120/200\n",
            "2720/2720 [==============================] - 2s 619us/step - loss: 0.1416 - accuracy: 0.9445 - val_loss: 0.2928 - val_accuracy: 0.9010\n",
            "Epoch 121/200\n",
            "2720/2720 [==============================] - 2s 612us/step - loss: 0.1302 - accuracy: 0.9515 - val_loss: 0.2989 - val_accuracy: 0.9010\n",
            "Epoch 122/200\n",
            "2720/2720 [==============================] - 2s 595us/step - loss: 0.1498 - accuracy: 0.9426 - val_loss: 0.2818 - val_accuracy: 0.8977\n",
            "Epoch 123/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.1362 - accuracy: 0.9445 - val_loss: 0.2995 - val_accuracy: 0.9010\n",
            "Epoch 124/200\n",
            "2720/2720 [==============================] - 2s 595us/step - loss: 0.1351 - accuracy: 0.9478 - val_loss: 0.2688 - val_accuracy: 0.8944\n",
            "Epoch 125/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.1443 - accuracy: 0.9478 - val_loss: 0.2860 - val_accuracy: 0.8911\n",
            "Epoch 126/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.1522 - accuracy: 0.9371 - val_loss: 0.3064 - val_accuracy: 0.9010\n",
            "Epoch 127/200\n",
            "2720/2720 [==============================] - 2s 589us/step - loss: 0.1367 - accuracy: 0.9478 - val_loss: 0.2897 - val_accuracy: 0.9010\n",
            "Epoch 128/200\n",
            "2720/2720 [==============================] - 2s 592us/step - loss: 0.1408 - accuracy: 0.9452 - val_loss: 0.2964 - val_accuracy: 0.9043\n",
            "Epoch 129/200\n",
            "2720/2720 [==============================] - 2s 591us/step - loss: 0.1311 - accuracy: 0.9434 - val_loss: 0.3053 - val_accuracy: 0.8911\n",
            "Epoch 130/200\n",
            "2720/2720 [==============================] - 2s 586us/step - loss: 0.1211 - accuracy: 0.9540 - val_loss: 0.2755 - val_accuracy: 0.8944\n",
            "Epoch 131/200\n",
            "2720/2720 [==============================] - 2s 595us/step - loss: 0.1264 - accuracy: 0.9515 - val_loss: 0.2935 - val_accuracy: 0.8977\n",
            "Epoch 132/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.1283 - accuracy: 0.9518 - val_loss: 0.2917 - val_accuracy: 0.9010\n",
            "Epoch 133/200\n",
            "2720/2720 [==============================] - 2s 584us/step - loss: 0.1267 - accuracy: 0.9504 - val_loss: 0.2942 - val_accuracy: 0.9043\n",
            "Epoch 134/200\n",
            "2720/2720 [==============================] - 2s 620us/step - loss: 0.1383 - accuracy: 0.9460 - val_loss: 0.2921 - val_accuracy: 0.8977\n",
            "Epoch 135/200\n",
            "2720/2720 [==============================] - 2s 605us/step - loss: 0.1199 - accuracy: 0.9500 - val_loss: 0.2871 - val_accuracy: 0.9076\n",
            "Epoch 136/200\n",
            "2720/2720 [==============================] - 2s 604us/step - loss: 0.1244 - accuracy: 0.9507 - val_loss: 0.3056 - val_accuracy: 0.9076\n",
            "Epoch 137/200\n",
            "2720/2720 [==============================] - 2s 587us/step - loss: 0.1456 - accuracy: 0.9415 - val_loss: 0.2938 - val_accuracy: 0.8944\n",
            "Epoch 138/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.1359 - accuracy: 0.9441 - val_loss: 0.2852 - val_accuracy: 0.8977\n",
            "Epoch 139/200\n",
            "2720/2720 [==============================] - 2s 590us/step - loss: 0.1229 - accuracy: 0.9493 - val_loss: 0.2973 - val_accuracy: 0.8911\n",
            "Epoch 140/200\n",
            "2720/2720 [==============================] - 2s 591us/step - loss: 0.1200 - accuracy: 0.9511 - val_loss: 0.2699 - val_accuracy: 0.8944\n",
            "Epoch 141/200\n",
            "2720/2720 [==============================] - 2s 587us/step - loss: 0.1056 - accuracy: 0.9581 - val_loss: 0.3139 - val_accuracy: 0.8911\n",
            "Epoch 142/200\n",
            "2720/2720 [==============================] - 2s 577us/step - loss: 0.1215 - accuracy: 0.9548 - val_loss: 0.2727 - val_accuracy: 0.9043\n",
            "Epoch 143/200\n",
            "2720/2720 [==============================] - 2s 584us/step - loss: 0.1089 - accuracy: 0.9614 - val_loss: 0.2857 - val_accuracy: 0.9076\n",
            "Epoch 144/200\n",
            "2720/2720 [==============================] - 2s 578us/step - loss: 0.1034 - accuracy: 0.9599 - val_loss: 0.2625 - val_accuracy: 0.8977\n",
            "Epoch 145/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.1254 - accuracy: 0.9511 - val_loss: 0.2913 - val_accuracy: 0.8911\n",
            "Epoch 146/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.1204 - accuracy: 0.9581 - val_loss: 0.2834 - val_accuracy: 0.8845\n",
            "Epoch 147/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.1205 - accuracy: 0.9518 - val_loss: 0.2875 - val_accuracy: 0.8977\n",
            "Epoch 148/200\n",
            "2720/2720 [==============================] - 2s 600us/step - loss: 0.1112 - accuracy: 0.9529 - val_loss: 0.2896 - val_accuracy: 0.9109\n",
            "Epoch 149/200\n",
            "2720/2720 [==============================] - 2s 589us/step - loss: 0.1058 - accuracy: 0.9610 - val_loss: 0.2761 - val_accuracy: 0.9076\n",
            "Epoch 150/200\n",
            "2720/2720 [==============================] - 2s 596us/step - loss: 0.1148 - accuracy: 0.9566 - val_loss: 0.3002 - val_accuracy: 0.9010\n",
            "Epoch 151/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.1228 - accuracy: 0.9500 - val_loss: 0.2770 - val_accuracy: 0.8977\n",
            "Epoch 152/200\n",
            "2720/2720 [==============================] - 2s 590us/step - loss: 0.1008 - accuracy: 0.9610 - val_loss: 0.2691 - val_accuracy: 0.9076\n",
            "Epoch 153/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.1135 - accuracy: 0.9574 - val_loss: 0.2728 - val_accuracy: 0.8944\n",
            "Epoch 154/200\n",
            "2720/2720 [==============================] - 2s 588us/step - loss: 0.1084 - accuracy: 0.9570 - val_loss: 0.2751 - val_accuracy: 0.9043\n",
            "Epoch 155/200\n",
            "2720/2720 [==============================] - 2s 586us/step - loss: 0.1026 - accuracy: 0.9585 - val_loss: 0.2687 - val_accuracy: 0.8977\n",
            "Epoch 156/200\n",
            "2720/2720 [==============================] - 2s 570us/step - loss: 0.0998 - accuracy: 0.9618 - val_loss: 0.2729 - val_accuracy: 0.8911\n",
            "Epoch 157/200\n",
            "2720/2720 [==============================] - 2s 594us/step - loss: 0.1141 - accuracy: 0.9544 - val_loss: 0.2885 - val_accuracy: 0.9010\n",
            "Epoch 158/200\n",
            "2720/2720 [==============================] - 2s 592us/step - loss: 0.1132 - accuracy: 0.9533 - val_loss: 0.2604 - val_accuracy: 0.9010\n",
            "Epoch 159/200\n",
            "2720/2720 [==============================] - 2s 591us/step - loss: 0.1047 - accuracy: 0.9603 - val_loss: 0.2888 - val_accuracy: 0.8977\n",
            "Epoch 160/200\n",
            "2720/2720 [==============================] - 2s 590us/step - loss: 0.1020 - accuracy: 0.9610 - val_loss: 0.2654 - val_accuracy: 0.9010\n",
            "Epoch 161/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.1045 - accuracy: 0.9559 - val_loss: 0.2875 - val_accuracy: 0.8944\n",
            "Epoch 162/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.0884 - accuracy: 0.9625 - val_loss: 0.2672 - val_accuracy: 0.9109\n",
            "Epoch 163/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.1146 - accuracy: 0.9577 - val_loss: 0.2966 - val_accuracy: 0.9010\n",
            "Epoch 164/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.1179 - accuracy: 0.9588 - val_loss: 0.2887 - val_accuracy: 0.9175\n",
            "Epoch 165/200\n",
            "2720/2720 [==============================] - 2s 573us/step - loss: 0.1001 - accuracy: 0.9654 - val_loss: 0.2662 - val_accuracy: 0.8977\n",
            "Epoch 166/200\n",
            "2720/2720 [==============================] - 2s 582us/step - loss: 0.0918 - accuracy: 0.9658 - val_loss: 0.2708 - val_accuracy: 0.9010\n",
            "Epoch 167/200\n",
            "2720/2720 [==============================] - 2s 572us/step - loss: 0.0981 - accuracy: 0.9651 - val_loss: 0.2874 - val_accuracy: 0.8944\n",
            "Epoch 168/200\n",
            "2720/2720 [==============================] - 2s 586us/step - loss: 0.0940 - accuracy: 0.9665 - val_loss: 0.3122 - val_accuracy: 0.8944\n",
            "Epoch 169/200\n",
            "2720/2720 [==============================] - 2s 590us/step - loss: 0.1024 - accuracy: 0.9662 - val_loss: 0.3180 - val_accuracy: 0.8944\n",
            "Epoch 170/200\n",
            "2720/2720 [==============================] - 2s 574us/step - loss: 0.0985 - accuracy: 0.9669 - val_loss: 0.3001 - val_accuracy: 0.8977\n",
            "Epoch 171/200\n",
            "2720/2720 [==============================] - 2s 574us/step - loss: 0.1088 - accuracy: 0.9592 - val_loss: 0.2949 - val_accuracy: 0.8977\n",
            "Epoch 172/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.0770 - accuracy: 0.9713 - val_loss: 0.2823 - val_accuracy: 0.8911\n",
            "Epoch 173/200\n",
            "2720/2720 [==============================] - 2s 574us/step - loss: 0.0935 - accuracy: 0.9647 - val_loss: 0.2769 - val_accuracy: 0.8977\n",
            "Epoch 174/200\n",
            "2720/2720 [==============================] - 2s 585us/step - loss: 0.0843 - accuracy: 0.9658 - val_loss: 0.2767 - val_accuracy: 0.8944\n",
            "Epoch 175/200\n",
            "2720/2720 [==============================] - 2s 590us/step - loss: 0.0768 - accuracy: 0.9699 - val_loss: 0.2903 - val_accuracy: 0.8977\n",
            "Epoch 176/200\n",
            "2720/2720 [==============================] - 2s 564us/step - loss: 0.1005 - accuracy: 0.9625 - val_loss: 0.2720 - val_accuracy: 0.9043\n",
            "Epoch 177/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.0886 - accuracy: 0.9684 - val_loss: 0.2886 - val_accuracy: 0.9109\n",
            "Epoch 178/200\n",
            "2720/2720 [==============================] - 2s 573us/step - loss: 0.0866 - accuracy: 0.9676 - val_loss: 0.2798 - val_accuracy: 0.8977\n",
            "Epoch 179/200\n",
            "2720/2720 [==============================] - 2s 592us/step - loss: 0.0843 - accuracy: 0.9724 - val_loss: 0.2740 - val_accuracy: 0.8977\n",
            "Epoch 180/200\n",
            "2720/2720 [==============================] - 2s 583us/step - loss: 0.0938 - accuracy: 0.9662 - val_loss: 0.2681 - val_accuracy: 0.8845\n",
            "Epoch 181/200\n",
            "2720/2720 [==============================] - 2s 590us/step - loss: 0.0885 - accuracy: 0.9680 - val_loss: 0.2762 - val_accuracy: 0.8911\n",
            "Epoch 182/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.0862 - accuracy: 0.9665 - val_loss: 0.2588 - val_accuracy: 0.8944\n",
            "Epoch 183/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.0901 - accuracy: 0.9658 - val_loss: 0.2760 - val_accuracy: 0.9010\n",
            "Epoch 184/200\n",
            "2720/2720 [==============================] - 2s 574us/step - loss: 0.0846 - accuracy: 0.9669 - val_loss: 0.2731 - val_accuracy: 0.8878\n",
            "Epoch 185/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.0911 - accuracy: 0.9647 - val_loss: 0.2880 - val_accuracy: 0.8944\n",
            "Epoch 186/200\n",
            "2720/2720 [==============================] - 2s 574us/step - loss: 0.0934 - accuracy: 0.9632 - val_loss: 0.2768 - val_accuracy: 0.9109\n",
            "Epoch 187/200\n",
            "2720/2720 [==============================] - 2s 595us/step - loss: 0.0706 - accuracy: 0.9724 - val_loss: 0.2868 - val_accuracy: 0.8977\n",
            "Epoch 188/200\n",
            "2720/2720 [==============================] - 2s 575us/step - loss: 0.0839 - accuracy: 0.9699 - val_loss: 0.3142 - val_accuracy: 0.8911\n",
            "Epoch 189/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.0805 - accuracy: 0.9746 - val_loss: 0.2926 - val_accuracy: 0.9076\n",
            "Epoch 190/200\n",
            "2720/2720 [==============================] - 2s 579us/step - loss: 0.0845 - accuracy: 0.9673 - val_loss: 0.2974 - val_accuracy: 0.8977\n",
            "Epoch 191/200\n",
            "2720/2720 [==============================] - 2s 581us/step - loss: 0.0716 - accuracy: 0.9757 - val_loss: 0.2907 - val_accuracy: 0.8911\n",
            "Epoch 192/200\n",
            "2720/2720 [==============================] - 2s 576us/step - loss: 0.0855 - accuracy: 0.9636 - val_loss: 0.3019 - val_accuracy: 0.8944\n",
            "Epoch 193/200\n",
            "2720/2720 [==============================] - 2s 566us/step - loss: 0.0836 - accuracy: 0.9706 - val_loss: 0.2974 - val_accuracy: 0.9010\n",
            "Epoch 194/200\n",
            "2720/2720 [==============================] - 2s 589us/step - loss: 0.0781 - accuracy: 0.9673 - val_loss: 0.3181 - val_accuracy: 0.9043\n",
            "Epoch 195/200\n",
            "2720/2720 [==============================] - 2s 580us/step - loss: 0.0735 - accuracy: 0.9739 - val_loss: 0.2944 - val_accuracy: 0.9043\n",
            "Epoch 196/200\n",
            "2720/2720 [==============================] - 2s 571us/step - loss: 0.0897 - accuracy: 0.9669 - val_loss: 0.3407 - val_accuracy: 0.8944\n",
            "Epoch 197/200\n",
            "2720/2720 [==============================] - 2s 568us/step - loss: 0.0807 - accuracy: 0.9721 - val_loss: 0.3156 - val_accuracy: 0.8878\n",
            "Epoch 198/200\n",
            "2720/2720 [==============================] - 2s 571us/step - loss: 0.0870 - accuracy: 0.9680 - val_loss: 0.2883 - val_accuracy: 0.9010\n",
            "Epoch 199/200\n",
            "2720/2720 [==============================] - 2s 562us/step - loss: 0.0756 - accuracy: 0.9721 - val_loss: 0.2903 - val_accuracy: 0.9043\n",
            "Epoch 200/200\n",
            "2720/2720 [==============================] - 2s 570us/step - loss: 0.0678 - accuracy: 0.9765 - val_loss: 0.3048 - val_accuracy: 0.9010\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f8ee6eb6a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRNeeRvpiL89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00zybaPoLFhY",
        "colab_type": "text"
      },
      "source": [
        "# **K- Fold**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz37Ak9KLIGo",
        "colab_type": "code",
        "outputId": "96644753-7e52-4020-8ff2-5846d8942bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "yt = []\n",
        "yp = []\n",
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "for train_idx, val_idx in kf.split(X1):\n",
        "  X_train1 = X1[train_idx]\n",
        "  X_train2 = X2[train_idx]\n",
        "  X_train3 = X3[train_idx]\n",
        "  X_train4 = X4[train_idx]\n",
        "  y_train = y[train_idx]\n",
        "\n",
        "  X_val1 = X1[val_idx]\n",
        "  X_val2 = X2[val_idx]\n",
        "  X_val3 = X3[val_idx]\n",
        "  X_val4 = X4[val_idx]\n",
        "  y_val = y[val_idx]\n",
        "\n",
        "  inputA = Input(shape=(81,100)) # One\n",
        "  inputB = Input(shape=(80,100)) # two\n",
        "  inputC = Input(shape=(79,100)) # Three\n",
        "  inputD = Input(shape=(78,100)) # Four\n",
        "\n",
        "  a = Convolution1D(128, kernel_size=3, activation='relu', padding='same') (inputA)\n",
        "  a = Dropout(0.5) (a)\n",
        "  a = Convolution1D(64, kernel_size=3, activation='relu', padding='same') (a)\n",
        "  a = Dropout(0.5) (a)\n",
        "  a = Convolution1D(32, kernel_size=3, activation='relu', padding='same') (a)\n",
        "  a = Dropout(0.5) (a)\n",
        "  a = Flatten() (a)\n",
        "\n",
        "  b = Convolution1D(filters = 128, kernel_size= 3, activation='relu', padding='same') (inputB)\n",
        "  b = Dropout(0.50) (b)\n",
        "  b = Convolution1D(filters = 64, kernel_size = 3, activation='relu', padding='same') (b)\n",
        "  b = Dropout(0.50) (b)\n",
        "  b = Convolution1D(filters = 32, kernel_size = 3, activation='relu', padding='same') (b)\n",
        "  b = Dropout(0.50) (b)\n",
        "  b = Flatten() (b)\n",
        "\n",
        "  c = Convolution1D(256, kernel_size=3, activation='relu', padding='same') (inputC)\n",
        "  c = Dropout(0.5) (c)\n",
        "  c = Conv1D(128, kernel_size=3, activation='relu', padding='same') (c)\n",
        "  c = Dropout(0.5) (c)\n",
        "  c = Conv1D(64, kernel_size=3, activation='relu', padding='same') (c)\n",
        "  c = Dropout(0.5) (c)\n",
        "  c = Conv1D(32, kernel_size=3, activation='relu', padding='same') (c)\n",
        "  c = Dropout(0.5) (c)\n",
        "  c = Flatten() (c)\n",
        "\n",
        "  d = Convolution1D(128, kernel_size=3, activation='relu', padding='same') (inputD)\n",
        "  d = Dropout(0.5) (d)\n",
        "  d = Conv1D(64, kernel_size=3, activation='relu', padding='same') (d)\n",
        "  d = Dropout(0.5) (d)\n",
        "  d = Conv1D(32, kernel_size=3, activation='relu', padding='same') (d)\n",
        "  d = Dropout(0.5) (d)\n",
        "  d = Flatten() (d)\n",
        "\n",
        "  combined = concatenate([a, b, c, d])\n",
        "\n",
        "  h = Dropout(0.5) (combined)\n",
        "  h = Dense(128, activation='relu') (h)\n",
        "  h = Dropout(0.5) (h)\n",
        "  h = Dense(64, activation='relu') (h)\n",
        "  output = Dense(1, activation='sigmoid') (h)\n",
        "\n",
        "  model = Model(inputs=[inputA, inputB, inputC, inputD], outputs=output)\n",
        "  model.compile(Adam(lr=0.0003), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  model.fit([X_train1, X_train2, X_train3, X_train4], y_train, \n",
        "            validation_data=([X_val1, X_val2, X_val3, X_val4], y_val),\n",
        "            epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "\n",
        "  probabilities = model.predict([X_val1, X_val2, X_val3, X_val4])\n",
        "  predicted_classes = probabilities >= 0.5\n",
        "  predicted_classes = predicted_classes.astype(int)\n",
        "  yp.append(predicted_classes)\n",
        "  yt.append(y_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4576 samples, validate on 1144 samples\n",
            "Epoch 1/10\n",
            "4576/4576 [==============================] - 3s 713us/step - loss: 0.7719 - accuracy: 0.4952 - val_loss: 0.6884 - val_accuracy: 0.6110\n",
            "Epoch 2/10\n",
            "4576/4576 [==============================] - 2s 509us/step - loss: 0.6825 - accuracy: 0.5610 - val_loss: 0.5767 - val_accuracy: 0.7351\n",
            "Epoch 3/10\n",
            "4576/4576 [==============================] - 2s 503us/step - loss: 0.5564 - accuracy: 0.7310 - val_loss: 0.4884 - val_accuracy: 0.7736\n",
            "Epoch 4/10\n",
            "4576/4576 [==============================] - 2s 506us/step - loss: 0.5256 - accuracy: 0.7476 - val_loss: 0.4966 - val_accuracy: 0.7517\n",
            "Epoch 5/10\n",
            "4576/4576 [==============================] - 2s 502us/step - loss: 0.5211 - accuracy: 0.7509 - val_loss: 0.4778 - val_accuracy: 0.7841\n",
            "Epoch 6/10\n",
            "4576/4576 [==============================] - 2s 519us/step - loss: 0.4975 - accuracy: 0.7677 - val_loss: 0.4696 - val_accuracy: 0.7876\n",
            "Epoch 7/10\n",
            "4576/4576 [==============================] - 2s 506us/step - loss: 0.4915 - accuracy: 0.7743 - val_loss: 0.4689 - val_accuracy: 0.7937\n",
            "Epoch 8/10\n",
            "4576/4576 [==============================] - 2s 511us/step - loss: 0.4904 - accuracy: 0.7775 - val_loss: 0.4686 - val_accuracy: 0.7622\n",
            "Epoch 9/10\n",
            "4576/4576 [==============================] - 2s 513us/step - loss: 0.4691 - accuracy: 0.7832 - val_loss: 0.4509 - val_accuracy: 0.7998\n",
            "Epoch 10/10\n",
            "4576/4576 [==============================] - 2s 516us/step - loss: 0.4703 - accuracy: 0.7880 - val_loss: 0.4524 - val_accuracy: 0.7876\n",
            "Train on 4576 samples, validate on 1144 samples\n",
            "Epoch 1/10\n",
            "4576/4576 [==============================] - 4s 772us/step - loss: 0.7670 - accuracy: 0.5031 - val_loss: 0.6859 - val_accuracy: 0.6302\n",
            "Epoch 2/10\n",
            "4576/4576 [==============================] - 2s 524us/step - loss: 0.6495 - accuracy: 0.6075 - val_loss: 0.5387 - val_accuracy: 0.7535\n",
            "Epoch 3/10\n",
            "4576/4576 [==============================] - 2s 511us/step - loss: 0.5493 - accuracy: 0.7301 - val_loss: 0.4950 - val_accuracy: 0.7815\n",
            "Epoch 4/10\n",
            "4576/4576 [==============================] - 2s 524us/step - loss: 0.5195 - accuracy: 0.7570 - val_loss: 0.5196 - val_accuracy: 0.7561\n",
            "Epoch 5/10\n",
            "4576/4576 [==============================] - 2s 545us/step - loss: 0.5155 - accuracy: 0.7607 - val_loss: 0.4787 - val_accuracy: 0.7841\n",
            "Epoch 6/10\n",
            "4576/4576 [==============================] - 2s 541us/step - loss: 0.5032 - accuracy: 0.7681 - val_loss: 0.4746 - val_accuracy: 0.7850\n",
            "Epoch 7/10\n",
            "4576/4576 [==============================] - 3s 549us/step - loss: 0.4971 - accuracy: 0.7697 - val_loss: 0.4678 - val_accuracy: 0.7876\n",
            "Epoch 8/10\n",
            "4576/4576 [==============================] - 2s 532us/step - loss: 0.4803 - accuracy: 0.7861 - val_loss: 0.4870 - val_accuracy: 0.7727\n",
            "Epoch 9/10\n",
            "4576/4576 [==============================] - 2s 523us/step - loss: 0.4823 - accuracy: 0.7819 - val_loss: 0.4597 - val_accuracy: 0.7928\n",
            "Epoch 10/10\n",
            "4576/4576 [==============================] - 2s 519us/step - loss: 0.4740 - accuracy: 0.7869 - val_loss: 0.4509 - val_accuracy: 0.8016\n",
            "Train on 4576 samples, validate on 1144 samples\n",
            "Epoch 1/10\n",
            "4576/4576 [==============================] - 3s 741us/step - loss: 0.7468 - accuracy: 0.5009 - val_loss: 0.6913 - val_accuracy: 0.5437\n",
            "Epoch 2/10\n",
            "4576/4576 [==============================] - 2s 522us/step - loss: 0.6946 - accuracy: 0.5350 - val_loss: 0.6266 - val_accuracy: 0.6233\n",
            "Epoch 3/10\n",
            "4576/4576 [==============================] - 2s 512us/step - loss: 0.5748 - accuracy: 0.7045 - val_loss: 0.5569 - val_accuracy: 0.7448\n",
            "Epoch 4/10\n",
            "4576/4576 [==============================] - 2s 520us/step - loss: 0.5398 - accuracy: 0.7450 - val_loss: 0.4837 - val_accuracy: 0.7893\n",
            "Epoch 5/10\n",
            "4576/4576 [==============================] - 2s 510us/step - loss: 0.5111 - accuracy: 0.7590 - val_loss: 0.4693 - val_accuracy: 0.7893\n",
            "Epoch 6/10\n",
            "4576/4576 [==============================] - 2s 516us/step - loss: 0.4967 - accuracy: 0.7677 - val_loss: 0.4718 - val_accuracy: 0.7832\n",
            "Epoch 7/10\n",
            "4576/4576 [==============================] - 2s 516us/step - loss: 0.4888 - accuracy: 0.7721 - val_loss: 0.4575 - val_accuracy: 0.7823\n",
            "Epoch 8/10\n",
            "4576/4576 [==============================] - 2s 519us/step - loss: 0.4868 - accuracy: 0.7666 - val_loss: 0.4790 - val_accuracy: 0.7753\n",
            "Epoch 9/10\n",
            "4576/4576 [==============================] - 2s 513us/step - loss: 0.4870 - accuracy: 0.7747 - val_loss: 0.4610 - val_accuracy: 0.7745\n",
            "Epoch 10/10\n",
            "4576/4576 [==============================] - 2s 517us/step - loss: 0.4748 - accuracy: 0.7793 - val_loss: 0.4487 - val_accuracy: 0.7937\n",
            "Train on 4576 samples, validate on 1144 samples\n",
            "Epoch 1/10\n",
            "4576/4576 [==============================] - 3s 725us/step - loss: 0.7704 - accuracy: 0.5066 - val_loss: 0.6952 - val_accuracy: 0.4930\n",
            "Epoch 2/10\n",
            "4576/4576 [==============================] - 2s 500us/step - loss: 0.7074 - accuracy: 0.5122 - val_loss: 0.6863 - val_accuracy: 0.6477\n",
            "Epoch 3/10\n",
            "4576/4576 [==============================] - 2s 509us/step - loss: 0.6684 - accuracy: 0.5756 - val_loss: 0.5440 - val_accuracy: 0.7491\n",
            "Epoch 4/10\n",
            "4576/4576 [==============================] - 2s 517us/step - loss: 0.5586 - accuracy: 0.7275 - val_loss: 0.5110 - val_accuracy: 0.7622\n",
            "Epoch 5/10\n",
            "4576/4576 [==============================] - 2s 513us/step - loss: 0.5123 - accuracy: 0.7611 - val_loss: 0.5568 - val_accuracy: 0.7544\n",
            "Epoch 6/10\n",
            "4576/4576 [==============================] - 2s 518us/step - loss: 0.5170 - accuracy: 0.7635 - val_loss: 0.4907 - val_accuracy: 0.7771\n",
            "Epoch 7/10\n",
            "4576/4576 [==============================] - 2s 519us/step - loss: 0.5043 - accuracy: 0.7692 - val_loss: 0.4739 - val_accuracy: 0.7815\n",
            "Epoch 8/10\n",
            "4576/4576 [==============================] - 2s 519us/step - loss: 0.4875 - accuracy: 0.7762 - val_loss: 0.4749 - val_accuracy: 0.7841\n",
            "Epoch 9/10\n",
            "4576/4576 [==============================] - 2s 518us/step - loss: 0.4900 - accuracy: 0.7769 - val_loss: 0.4844 - val_accuracy: 0.7850\n",
            "Epoch 10/10\n",
            "4576/4576 [==============================] - 2s 510us/step - loss: 0.4754 - accuracy: 0.7815 - val_loss: 0.4420 - val_accuracy: 0.7972\n",
            "Train on 4576 samples, validate on 1144 samples\n",
            "Epoch 1/10\n",
            "4576/4576 [==============================] - 3s 714us/step - loss: 0.7641 - accuracy: 0.5133 - val_loss: 0.6858 - val_accuracy: 0.6049\n",
            "Epoch 2/10\n",
            "4576/4576 [==============================] - 2s 501us/step - loss: 0.6977 - accuracy: 0.5413 - val_loss: 0.6477 - val_accuracy: 0.5245\n",
            "Epoch 3/10\n",
            "4576/4576 [==============================] - 2s 510us/step - loss: 0.5969 - accuracy: 0.6807 - val_loss: 0.4798 - val_accuracy: 0.7937\n",
            "Epoch 4/10\n",
            "4576/4576 [==============================] - 2s 494us/step - loss: 0.5371 - accuracy: 0.7448 - val_loss: 0.4532 - val_accuracy: 0.7946\n",
            "Epoch 5/10\n",
            "4576/4576 [==============================] - 2s 504us/step - loss: 0.5179 - accuracy: 0.7574 - val_loss: 0.4629 - val_accuracy: 0.7841\n",
            "Epoch 6/10\n",
            "4576/4576 [==============================] - 2s 513us/step - loss: 0.5079 - accuracy: 0.7651 - val_loss: 0.4383 - val_accuracy: 0.8016\n",
            "Epoch 7/10\n",
            "4576/4576 [==============================] - 2s 514us/step - loss: 0.4950 - accuracy: 0.7745 - val_loss: 0.4237 - val_accuracy: 0.8129\n",
            "Epoch 8/10\n",
            "4576/4576 [==============================] - 2s 503us/step - loss: 0.4885 - accuracy: 0.7791 - val_loss: 0.4174 - val_accuracy: 0.8243\n",
            "Epoch 9/10\n",
            "4576/4576 [==============================] - 2s 503us/step - loss: 0.4744 - accuracy: 0.7819 - val_loss: 0.4215 - val_accuracy: 0.8191\n",
            "Epoch 10/10\n",
            "4576/4576 [==============================] - 2s 495us/step - loss: 0.4687 - accuracy: 0.7891 - val_loss: 0.4209 - val_accuracy: 0.8173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM-P5_u-LK_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true = []\n",
        "pred = []\n",
        "a=0\n",
        "for i in yt:\n",
        "  b=0\n",
        "  for j in i:\n",
        "    true.append(j)\n",
        "    pred.append(yp[a][b])\n",
        "    b=+1\n",
        "  a+=1\n",
        "    \n",
        "y_true = np.asarray(true)\n",
        "y_pred = np.asarray(pred)\n",
        "\n",
        "print('Acc: ', accuracy_score(y_true, y_pred) * 100)\n",
        "print('MCC: ', matthews_corrcoef(y_true, y_pred) * 100)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print('Sn:  ', (cm[1,1] / (cm[1,1] + cm[1,0])) * 100)\n",
        "print('Sp:  ', (cm[0,0] / (cm[0,0] + cm[0,1])) * 100)\n",
        "\n",
        "p = precision_recall_fscore_support(y_true, y_pred)\n",
        "print('pre_ 0: ', p[0][0] * 100)\n",
        "print('pre_ 1: ', p[0][1] * 100)\n",
        "print('Pre_weighted: ', precision_score(y_true, y_pred, average='weighted') * 100)\n",
        "\n",
        "print('F-score: ', np.mean(p[2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4Ed2SpTRPWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}